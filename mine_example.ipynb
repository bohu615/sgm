{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SciencePlots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.reload_library()\n",
    "\n",
    "# plt.style.use('science')\n",
    "# plt.style.use(['science','no-latex', 'high-vis', 'notebook'])\n",
    "# plt.rcParams[\"figure.figsize\"] = [6,4]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "def compute_gram_matrix_nocoeff(distance_matrix, sigma):\n",
    "    #G_matrix = np.exp(-distance_matrix/(2*sigma**2))/np.sqrt(2*np.pi*sigma**2)\n",
    "    G_matrix = np.exp(-distance_matrix/(2*sigma**2))\n",
    "    return G_matrix\n",
    "\n",
    "def compute_gram_matrix(distance_matrix, sigma):\n",
    "    G_matrix = np.exp(-distance_matrix/(2*sigma**2))/np.sqrt(2*np.pi*sigma**2)\n",
    "    return G_matrix\n",
    "\n",
    "def delete_diagonal(A):\n",
    "    return A[~np.eye(A.shape[0],dtype=bool)].reshape(A.shape[0],-1)\n",
    "\n",
    "def compute_QMI(x, z, step = 1):\n",
    "    \n",
    "    x = np.expand_dims(x, 1)\n",
    "    z = np.expand_dims(z, 1)\n",
    "\n",
    "    distance_x = (np.expand_dims(x, 1) - np.expand_dims(x, 0))**2\n",
    "    distance_z = (np.expand_dims(z, 1) - np.expand_dims(z, 0))**2\n",
    "    \n",
    "    distance_x_1_2 = np.sum((distance_x**2), 2)\n",
    "    distance_y_1_2 = np.sum(distance_z**2, 2)\n",
    "\n",
    "    step = 1\n",
    "\n",
    "    G_matrix_x_1_2 = compute_gram_matrix(distance_x_1_2, step)\n",
    "    G_matrix_y_1_2 = compute_gram_matrix(distance_y_1_2, step)\n",
    "    \n",
    "    V_f = np.mean(G_matrix_x_1_2*G_matrix_y_1_2) \n",
    "    V_x = np.mean(G_matrix_x_1_2)\n",
    "    V_y = np.mean(G_matrix_y_1_2)\n",
    "\n",
    "    QMI = (-np.log2(V_x)) + (-np.log2(V_y)) - (-np.log2(V_f)) \n",
    "    \n",
    "    return QMI\n",
    "\n",
    "def compute_E_QMI(x, z, step = 1, batch_size = 300):\n",
    "    \n",
    "    x = x.reshape(-1, 1)\n",
    "    z = z.reshape(-1, 1)\n",
    "    \n",
    "    #np.random.seed(4)\n",
    "    \n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train = x[batch, :]\n",
    "    y_train = z[batch, :]\n",
    "\n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train_2 = x[batch, :]\n",
    "    y_train_2 = z[batch, :]\n",
    "    \n",
    "    L1_distance = x_train - x_train_2\n",
    "    y_L1_distance = y_train - y_train_2\n",
    "\n",
    "    distance_x_1_2 = np.sum(L1_distance**2, 1)\n",
    "    distance_y_1_2 = np.sum(y_L1_distance**2, 1)\n",
    "\n",
    "    step = 1\n",
    "\n",
    "    G_matrix_x_1_2 = compute_gram_matrix(distance_x_1_2, step) \n",
    "    G_matrix_y_1_2 = compute_gram_matrix(distance_y_1_2, step) \n",
    "\n",
    "    #G_matrix_x_1_2 = G_matrix_x_1_2 - torch.mean(G_matrix_x_1_2) + torch.std(G_matrix_x_1_2)\n",
    "    #G_matrix_y_1_2 = G_matrix_y_1_2 - y_mean + y_std\n",
    "\n",
    "    V_f = np.mean(G_matrix_x_1_2*G_matrix_y_1_2) \n",
    "    V_x = np.mean(G_matrix_x_1_2)\n",
    "    V_y = np.mean(G_matrix_y_1_2)\n",
    "\n",
    "    QMI = (-np.log2(V_x)) + (-np.log2(V_y)) - (-np.log2(V_f)) \n",
    "    \n",
    "    return QMI\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    mean = np.mean(tensor, 0)\n",
    "    tensor = tensor-mean\n",
    "    std = np.std(tensor, 0)\n",
    "    tensor = tensor/std\n",
    "    tensor = tensor\n",
    "    return tensor\n",
    "\n",
    "def compute_E_QMI_type1(x, z, step = 1, batch_size = 300):\n",
    "    \n",
    "    x = x.reshape(-1, 1)\n",
    "    z = z.reshape(-1, 1)\n",
    "\n",
    "    z = ((z-np.mean(z))/np.std(z))/np.sqrt(2)\n",
    "    \n",
    "    #np.random.seed(4)\n",
    "    \n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train = x[batch, :]\n",
    "    y_train = z[batch, :]\n",
    "\n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train_2 = x[batch, :]\n",
    "    y_train_2 = z[batch, :]\n",
    "    \n",
    "    L1_distance = x_train - x_train_2\n",
    "    L1_distance = normalize_tensor(L1_distance)\n",
    "    y_L1_distance = y_train - y_train_2\n",
    "\n",
    "    distance_x_1_2 = np.sum(L1_distance**2, 1)\n",
    "    distance_y_1_2 = np.sum(y_L1_distance**2, 1)\n",
    "\n",
    "    step = 1\n",
    "\n",
    "    G_matrix_x_1_2 = compute_gram_matrix(distance_x_1_2, step) \n",
    "    G_matrix_y_1_2 = compute_gram_matrix(distance_y_1_2, step) \n",
    "\n",
    "    #G_matrix_x_1_2 = G_matrix_x_1_2 - np.mean(G_matrix_x_1_2) + np.std(G_matrix_x_1_2)\n",
    "    #G_matrix_y_1_2 = G_matrix_y_1_2 - np.mean(G_matrix_y_1_2) + np.std(G_matrix_y_1_2)\n",
    "\n",
    "    V_f = np.mean(G_matrix_x_1_2*G_matrix_y_1_2) \n",
    "    V_x = np.mean(G_matrix_x_1_2)\n",
    "    V_y = np.mean(G_matrix_y_1_2)\n",
    "\n",
    "    QMI = (-np.log2(V_x)) + (-np.log2(V_y))  - (-np.log2(V_f)) \n",
    "    \n",
    "    return QMI\n",
    "\n",
    "def compute_E_QMI_type2(x, z, step = 1, batch_size = 300):\n",
    "    \n",
    "    x = x.reshape(-1, 1)\n",
    "    z = z.reshape(-1, 1)\n",
    "\n",
    "    #np.random.seed(4)\n",
    "    \n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train = x[batch, :]\n",
    "    y_train = z[batch, :]\n",
    "\n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train_2 = x[batch, :]\n",
    "    y_train_2 = z[batch, :]\n",
    "    \n",
    "    L1_distance = x_train - x_train_2\n",
    "    y_L1_distance = y_train - y_train_2\n",
    "\n",
    "    distance_x_1_2 = np.sum(L1_distance**2, 1)\n",
    "    distance_y_1_2 = np.sum(y_L1_distance**2, 1)\n",
    "\n",
    "    step = 1\n",
    "\n",
    "    G_matrix_x_1_2 = compute_gram_matrix(distance_x_1_2, step) \n",
    "    G_matrix_y_1_2 = compute_gram_matrix(distance_y_1_2, step) \n",
    "\n",
    "    G_matrix_x_1_2 = G_matrix_x_1_2 - np.mean(G_matrix_x_1_2) + np.std(G_matrix_x_1_2)\n",
    "    G_matrix_y_1_2 = G_matrix_y_1_2 - np.mean(G_matrix_y_1_2) + np.std(G_matrix_y_1_2)\n",
    "\n",
    "    V_f = np.mean(G_matrix_x_1_2*G_matrix_y_1_2) \n",
    "    V_x = np.mean(G_matrix_x_1_2)\n",
    "    V_y = np.mean(G_matrix_y_1_2)\n",
    "\n",
    "    QMI = (-np.log2(V_x)) + (-np.log2(V_y)) - (-np.log2(V_f)) \n",
    "    \n",
    "    return QMI\n",
    "\n",
    "def compute_E_QMI_coeff(x, z, step = 1, batch_size = 300):\n",
    "    \n",
    "    x = x.reshape(-1, 1)\n",
    "    z = z.reshape(-1, 1)\n",
    "\n",
    "    np.random.seed(4)\n",
    "    \n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train = x[batch, :]\n",
    "    y_train = z[batch, :]\n",
    "\n",
    "    batch = np.random.choice(x.shape[0], batch_size)\n",
    "    x_train_2 = x[batch, :]\n",
    "    y_train_2 = z[batch, :]\n",
    "    \n",
    "    L1_distance = x_train - x_train_2\n",
    "    y_L1_distance = y_train - y_train_2\n",
    "\n",
    "    distance_x_1_2 = np.sum(L1_distance**2, 1)\n",
    "    distance_y_1_2 = np.sum(y_L1_distance**2, 1)\n",
    "\n",
    "    step = 1\n",
    "\n",
    "    G_matrix_x_1_2 = np.copy(compute_gram_matrix_nocoeff(distance_x_1_2, step))\n",
    "    G_matrix_y_1_2 = np.copy(compute_gram_matrix_nocoeff(distance_y_1_2, step))\n",
    "    V_x = np.mean(G_matrix_x_1_2)\n",
    "    V_y = np.mean(G_matrix_y_1_2)\n",
    "\n",
    "    step = 1*np.sqrt(2)\n",
    "        \n",
    "    G_matrix_x_1_2 = compute_gram_matrix_nocoeff(distance_x_1_2, step) \n",
    "    G_matrix_y_1_2 = compute_gram_matrix_nocoeff(distance_y_1_2, step) \n",
    "    V_x_d2 = np.mean(G_matrix_x_1_2)\n",
    "    V_y_d2 = np.mean(G_matrix_y_1_2)\n",
    "    \n",
    "    V_f = np.mean(G_matrix_x_1_2*G_matrix_y_1_2)\n",
    "    \n",
    "    QMI = ((-np.log2(V_x)) + (-np.log2(V_y)) - (-np.log2(V_f)))\n",
    "    min = (-np.log2(V_x)) + (-np.log2(V_y)) - (-np.log2(V_x_d2)) - (-np.log2(V_y_d2))\n",
    "    max = (-np.log2(V_x)+-np.log2(V_y))/2\n",
    "        \n",
    "    return (QMI-min)/(max-min)\n",
    "\n",
    "def compute_E_CSQMI_nd(x, z, A, B, batch_size = 300, runs = 1000):\n",
    "\n",
    "    V_f = 0\n",
    "    V_x = 0\n",
    "    V_y = 0\n",
    "\n",
    "    for run in range(0, runs):\n",
    "\n",
    "        batch = np.random.choice(x.shape[0], batch_size)\n",
    "        x_train = x[batch, :]\n",
    "        y_train = z[batch, :]\n",
    "\n",
    "        batch = np.random.choice(x.shape[0], batch_size)\n",
    "        x_train_2 = x[batch, :]\n",
    "        y_train_2 = z[batch, :]\n",
    "\n",
    "        batch = np.random.choice(x.shape[0], batch_size)\n",
    "        y_train_3 = z[batch, :]\n",
    "\n",
    "        batch = np.random.choice(x.shape[0], batch_size)\n",
    "        y_train_4 = z[batch, :]\n",
    "\n",
    "        G_matrix_x_1_2 = gaussian_nd(x_train, x_train_2, 2*A)\n",
    "        G_matrix_y_1_2 = gaussian_nd(y_train, y_train_2, 2*B)\n",
    "\n",
    "        G_matrix_y_1_3 = gaussian_nd(y_train, y_train_3, 2*B)\n",
    "        G_matrix_y_3_4 = gaussian_nd(y_train_3, y_train_4, 2*B)\n",
    "\n",
    "        V_f = (V_f*run + np.mean(G_matrix_x_1_2*G_matrix_y_1_3))/(run+1)\n",
    "        V_x = (V_x*run + np.mean(G_matrix_x_1_2*G_matrix_y_1_2))/(run+1)\n",
    "        V_y = (V_y*run + np.mean(G_matrix_x_1_2*G_matrix_y_3_4))/(run+1)\n",
    "\n",
    "    QMI = (-np.log2(V_f)) + (1/2)*np.log2(V_x) + (1/2)*np.log2(V_y)\n",
    "    return QMI\n",
    "\n",
    "def compute_TRUE_CSQMI_nd(center_x, center_y, A, B):\n",
    "\n",
    "    K = center_x.shape[0]\n",
    "    dim = center_y.shape[1]\n",
    "\n",
    "    V_f = 0\n",
    "    V_x = 0\n",
    "    V_y = 0\n",
    "\n",
    "    gram_x = (center_x.reshape(K, 1, dim) - center_x.reshape(1, K, dim)).reshape(-1, dim)\n",
    "    gram_y = (center_y.reshape(K, 1, dim) - center_y.reshape(1, K, dim)).reshape(-1, dim)\n",
    "\n",
    "    G_matrix_x = gaussian_nd(gram_x, 0, 2*A).reshape(K, K)\n",
    "    G_matrix_y = gaussian_nd(gram_y, 0, 2*B).reshape(K, K)\n",
    "\n",
    "    V_x = np.mean(G_matrix_x*G_matrix_y)\n",
    "    V_y = np.mean(G_matrix_x)*np.mean(G_matrix_y)\n",
    "    V_f = np.mean(G_matrix_x.reshape(K, K, 1)*G_matrix_y.reshape(K, K))\n",
    "\n",
    "    CSQMI = (-np.log2(V_f)) + (1/2)*np.log2(V_x) + (1/2)*np.log2(V_y)\n",
    "    return CSQMI\n",
    "\n",
    "def gaussian_1d(input, m, sigma):\n",
    "    return np.exp(-(input-m)**2/(2*sigma**2))/np.sqrt(2*np.pi*sigma**2)\n",
    "\n",
    "def gaussian_nd(input, m, sigma):\n",
    "    k = sigma.shape[0]\n",
    "    det = np.linalg.det(sigma)\n",
    "    inv = np.linalg.pinv(sigma)\n",
    "    \n",
    "    return ((2*np.pi)**(-k/2))*det**(-1/2)*np.exp(-(1/2)*np.sum((input-m)@inv*(input-m), 1))\n",
    "\n",
    "def identity(x):\n",
    "    return np.eye(x.shape[1])\n",
    "\n",
    "def generate_gauss_samples_nd(center_x, center_y, COV, samples_per_class=3000):\n",
    "    \n",
    "    MEAN = np.concatenate([center_x, center_y], 1)    \n",
    "    component_ = []\n",
    "    num_class = MEAN.shape[0]\n",
    "    for i in range(0, num_class):\n",
    "        samples = np.random.multivariate_normal(MEAN[i], COV, samples_per_class)\n",
    "        component_.append(samples)\n",
    "    component_ = np.array(component_).reshape(-1, COV.shape[0])\n",
    "    \n",
    "    return component_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATE A LOT DATASET\n",
    "min = 0\n",
    "max = 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def laplacian_pdf(input, mean, scale):\n",
    "     return (1/(2*scale))*np.exp(-np.abs(input-mean)/scale)\n",
    "\n",
    "def gaussian_nd(input, m, sigma):\n",
    "    k = sigma.shape[0]\n",
    "    det = np.linalg.det(sigma)\n",
    "    inv = np.linalg.pinv(sigma)\n",
    "    \n",
    "    return ((2*np.pi)**(-k/2))*det**(-1/2)*np.exp(-(1/2)*np.sum((input-m)@inv*(input-m), 1))\n",
    "\n",
    "def construct_contour(centers, weights, learned_variance, interp=100):\n",
    "    QMI_TRUE_LIST = []\n",
    "    interp = 100\n",
    "    delta = (max-min)/interp\n",
    "\n",
    "    x_axis = np.linspace(min, max, interp)\n",
    "    y_axis = np.linspace(min, max, interp)\n",
    "    xv, yv = np.meshgrid(x_axis,y_axis)\n",
    "\n",
    "    input = np.array((xv, yv)).reshape(2, -1).T\n",
    "\n",
    "    gaussian_plot_joint_ = []\n",
    "    gaussian_plot_split_x_ = []\n",
    "    gaussian_plot_split_y_ = []\n",
    "\n",
    "    #centers = np.concatenate((center_x, center_y), 1)\n",
    "    difference = input.reshape(1, -1, 2) - centers.reshape(-1, 1, 2)\n",
    "\n",
    "    for i in range(0, centers.shape[0]):\n",
    "        gaussian_plot_joint_.append(weights[i]*gaussian_nd(difference[i], 0, np.array([[learned_variance[i,0], 0], [0, learned_variance[i,1]]])).reshape(interp, interp))\n",
    "\n",
    "    gaussian_plot_joint = np.mean(np.array(gaussian_plot_joint_), 0)*delta*delta\n",
    "    \n",
    "    return gaussian_plot_joint\n",
    "\n",
    "def construct_pdf_mix_mixture(center_x, center_y, COV, samples_per_class=3000):\n",
    "    \n",
    "    MEAN = np.concatenate([center_x, center_y], 1)   \n",
    "    COV_ = []\n",
    "    weights_ = []\n",
    "    component_ = []\n",
    "    num_class = MEAN.shape[0]\n",
    "    \n",
    "    interp = 1000\n",
    "    max = 1\n",
    "    min = 0\n",
    "    delta = (max-min)/interp\n",
    "\n",
    "    x_axis = np.linspace(min, max, interp)\n",
    "    y_axis = np.linspace(min, max, interp)\n",
    "    xv, yv = np.meshgrid(x_axis,y_axis)\n",
    "\n",
    "    input = np.array((xv, yv)).reshape(2, -1).T\n",
    "    pdf_map = np.zeros((interp, interp))\n",
    "    \n",
    "    for i in range(0, num_class):\n",
    "        COV[0, 0] = np.random.uniform(0.0002, 0.0008, 1)[0]\n",
    "        COV[1, 1] = np.random.uniform(0.0002, 0.0008, 1)[0]\n",
    "        weights = np.random.uniform(0, 1, 1)[0]+0.5\n",
    "        \n",
    "        rv = np.random.choice(3, 1)[0]\n",
    "        \n",
    "        # if rv==0:\n",
    "        samples = np.random.multivariate_normal(MEAN[i], COV, int(weights*samples_per_class))\n",
    "        pdf_map = pdf_map + (weights*gaussian_nd(input - MEAN[i], 0, COV)).reshape(interp, interp)\n",
    "            \n",
    "        # if rv==1:\n",
    "        #     dim_1 = np.random.uniform(MEAN[i, 0]-COV[0, 0]*10, MEAN[i, 0]+COV[0, 0]*10, int(weights*samples_per_class))\n",
    "        #     dim_2 = np.random.uniform(MEAN[i, 1]-COV[1, 1]*10, MEAN[i, 1]+COV[1, 1]*10, int(weights*samples_per_class))\n",
    "        #     samples = np.array((dim_1, dim_2)).T\n",
    "        #     pdf = (input[:, 0]>(MEAN[i, 0]-COV[0, 0]*10))*(input[:, 0]<(MEAN[i, 0]+COV[0, 0]*10))\n",
    "        #     pdf = pdf*(input[:, 1]>(MEAN[i, 1]-COV[1, 1]*10))*(input[:, 1]<(MEAN[i, 1]+COV[1, 1]*10))\n",
    "        #     pdf_map = pdf_map + (pdf/(400*COV[0, 0]*COV[1, 1])).reshape(interp, interp)\n",
    "            \n",
    "        # if rv==2:\n",
    "        #     dim_1 = np.random.laplace(MEAN[i, 0], scale=COV[0, 0]*10, size=int(weights*samples_per_class))\n",
    "        #     dim_2 = np.random.laplace(MEAN[i, 1], scale=COV[1, 1]*10, size=int(weights*samples_per_class))\n",
    "        #     samples = np.array((dim_1, dim_2)).T\n",
    "        #     pdf_dim_1 = laplacian_pdf(input[:, 0], MEAN[i, 0], COV[0, 0]*10)\n",
    "        #     pdf_dim_2 = laplacian_pdf(input[:, 1], MEAN[i, 1], COV[1, 1]*10)\n",
    "            \n",
    "        #     pdf_map = pdf_map + weights*(pdf_dim_1*pdf_dim_2).reshape(interp, interp)\n",
    "            \n",
    "        component_.append(samples)\n",
    "        COV_.append(np.copy(COV))\n",
    "        weights_.append(np.copy(weights))\n",
    "    component_ = np.concatenate(component_).reshape(-1, COV.shape[0])\n",
    "    \n",
    "    return component_, MEAN, np.array(COV_), np.array(weights_), pdf_map, delta\n",
    "\n",
    "#### GENERATING DATASET!\n",
    "K = 5\n",
    "dim = 1\n",
    "\n",
    "seed_ = [4, 5, 6, 7, 8]\n",
    "samples_list = []\n",
    "cov_list = []\n",
    "mean_list = []\n",
    "weight_list = []\n",
    "\n",
    "for seed in seed_:\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    center_x = np.array([np.random.uniform(0.2, 0.8, K)]*dim).T\n",
    "    center_y = np.array([np.random.uniform(0.2, 0.8, K)]*dim).T\n",
    "\n",
    "    how_many_samples = 100000\n",
    "\n",
    "    var = 0.001\n",
    "\n",
    "    dim = 1\n",
    "    # JOINT DISTRIBUTION\n",
    "    COV = np.eye(dim*2)*var\n",
    "\n",
    "    QMI_LIST = []\n",
    "    np.random.seed(seed)\n",
    "    samples, MEAN, COV_, weights_, pdf_map, delta = construct_pdf_mix_mixture(center_x, center_y, COV, samples_per_class=how_many_samples)\n",
    "    pdf_map = pdf_map/np.sum(pdf_map)\n",
    "\n",
    "    samples_list.append(samples)\n",
    "    cov_list.append(COV_)\n",
    "    mean_list.append(MEAN)\n",
    "    weight_list.append(weights_/np.sum(weights_))\n",
    "\n",
    "    # true_entropy = np.sqrt(np.sum(pdf_map*pdf_map)/(delta*delta))\n",
    "    # print('entropy:', true_entropy)\n",
    "    # plt.imshow(np.log(pdf_map+1e-5), origin='lower', extent=[min, max, min, max])\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nd(input, m, sigma):\n",
    "    k = sigma.shape[0]\n",
    "    det = np.linalg.det(sigma)\n",
    "    inv = np.linalg.pinv(sigma)\n",
    "    \n",
    "    return ((2*np.pi)**(-k/2))*det**(-1/2)*np.exp(-(1/2)*np.sum((input-m)@inv*(input-m), 1))\n",
    "\n",
    "# def gaussian_nd_save_some(input, m, sigma):\n",
    "#     det = (sigma[:, 0]*sigma[:, 1])\n",
    "#     inv = 1/sigma\n",
    "    \n",
    "#     return ((2*np.pi)**(-2/2))*det**(-1/2)*np.exp(-(1/2)*np.sum((input-m)*inv*(input-m), 1))\n",
    "\n",
    "def gaussian_nd_save_some(MEAN, VARIANCE):\n",
    "    bs = VARIANCE.shape[0]\n",
    "    dim = VARIANCE.shape[1]\n",
    "\n",
    "    det = VARIANCE[:, 0]\n",
    "    for i in range(1, dim):\n",
    "        det = det*VARIANCE[:, i]    \n",
    "        \n",
    "    product = np.sum(((MEAN.reshape(-1, dim)*(1/VARIANCE).reshape(-1, dim))*MEAN.reshape(-1, dim)), 1)\n",
    "    \n",
    "    return ((2*np.pi)**(-dim/2))*det**(-1/2)*np.exp(-(1/2)*product)\n",
    "\n",
    "def gaussian_1d(input, m, sigma):\n",
    "    det = sigma\n",
    "    inv = 1/sigma\n",
    "        \n",
    "    input = input.reshape(-1, 1)\n",
    "    m = m.reshape(1, -1)\n",
    "\n",
    "    return ((2*np.pi)**(-1/2))*det**(-1/2)*np.exp(-(1/2)*((input-m)**2*inv))\n",
    "\n",
    "def compute_E_CSQMI_nd(gmmmm, gmvvv, gmwww):\n",
    "\n",
    "    V_f = 0\n",
    "    V_x = 0\n",
    "    V_y = 0\n",
    "\n",
    "    meandiff = (gmmmm[:, 0].reshape(-1, 1) - gmmmm[:, 0].reshape(1, -1)).reshape(-1, 1)\n",
    "    vardiff = (gmvvv[:, 0, 0].reshape(-1, 1) + gmvvv[:, 0, 0].reshape(1, -1)).reshape(-1, 1)\n",
    "    weight_diff = (gmwww.reshape(-1, 1)*gmwww.reshape(1, -1)).reshape(-1)\n",
    "    G_matrix_x = weight_diff*gaussian_nd_save_some(meandiff, vardiff)\n",
    "\n",
    "    K = int(np.sqrt(G_matrix_x.shape[0]))\n",
    "\n",
    "    meandiff = (gmmmm[:, 1].reshape(-1, 1) - gmmmm[:, 1].reshape(1, -1)).reshape(-1, 1)\n",
    "    vardiff = (gmvvv[:, 1, 1].reshape(-1, 1) + gmvvv[:, 1, 1].reshape(1, -1)).reshape(-1, 1)\n",
    "    weight_diff = (gmwww.reshape(-1, 1)*gmwww.reshape(1, -1)).reshape(-1)\n",
    "    G_matrix_y = weight_diff*gaussian_nd_save_some(meandiff, vardiff)\n",
    "\n",
    "    V_x = np.mean(G_matrix_x*G_matrix_y)\n",
    "    V_y = np.mean(G_matrix_x)*np.mean(G_matrix_y)\n",
    "    V_f = np.mean(G_matrix_x.reshape(K, K, 1)*G_matrix_y.reshape(K, K))\n",
    "\n",
    "    CSQMI = (-np.log2(V_f)) + (1/2)*np.log2(V_x) + (1/2)*np.log2(V_y)\n",
    "    return CSQMI\n",
    "\n",
    "for i in range(0, 5):\n",
    "  csqmi = compute_E_CSQMI_nd(mean_list[i], cov_list[i], weight_list[i])\n",
    "  print('True QMI (EXP #{0})'.format(i+1), csqmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) SGM - QMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "#torch.cuda.set_device(1)\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "    return np.random.uniform(min, max, batch)\n",
    "\n",
    "def gaussian_nd_pytorch(MEAN, VARIANCE):\n",
    "    bs = VARIANCE.shape[0]\n",
    "    dim = VARIANCE.shape[1]\n",
    "\n",
    "    det = VARIANCE[:, 0]\n",
    "    for i in range(1, dim):\n",
    "        det = det*VARIANCE[:, i]    \n",
    "        \n",
    "    product = torch.sum(((MEAN.reshape(-1, dim)*(1/VARIANCE).view(-1, dim))*MEAN.reshape(-1, dim)), 1)\n",
    "    \n",
    "    return ((2*np.pi)**(-dim/2))*det**(-1/2)*torch.exp(-(1/2)*product)\n",
    "\n",
    "def sample_c(batchsize=32, dis_category=5):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def generate_laplacian(center_x, center_y,  scale=0.01, samples_per_class=3000):\n",
    "    \n",
    "    component_ = []\n",
    "    num_class = MEAN.shape[0]\n",
    "\n",
    "    for i in range(0, num_class):\n",
    "        dim_1 = np.random.laplace(MEAN[i, 0], scale=scale, size=samples_per_class)\n",
    "        dim_2 = np.random.laplace(MEAN[i, 1], scale=scale, size=samples_per_class)\n",
    "        samples = np.array((dim_1, dim_2)).T\n",
    "        component_.append(samples)\n",
    "\n",
    "    component_ = np.array(component_).reshape(-1, MEAN.shape[1])    \n",
    "    \n",
    "    return component_\n",
    "\n",
    "def generate_uniform(center_x, center_y,  length=0.5, samples_per_class=3000):\n",
    "    component_ = []\n",
    "    \n",
    "    num_class = MEAN.shape[0]\n",
    "    for i in range(0, num_class):\n",
    "        x_1 = np.random.uniform(center_x[i], center_x[i]+length, samples_per_class)\n",
    "        x_3 = np.random.uniform(center_y[i], center_y[i]+length, samples_per_class)\n",
    "        x = np.array((x_1, x_3)).T\n",
    "        component_.append(x)\n",
    "            \n",
    "    component_ = np.concatenate(component_, 0)\n",
    "    return component_\n",
    "\n",
    "\n",
    "class DIS_MOG(nn.Module):\n",
    "    def __init__(self, rand, HIDDEN, dim):\n",
    "        super(DIS_MOG, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc1_w = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 1\n",
    "        self.sum_dim_var = 1\n",
    "        self.sum_dim_weights = 30\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        x_m = self.bn3_(torch.sigmoid((self.fc3_(x_m))))\n",
    "        x_m = (torch.sigmoid((self.fc33_(x_m))))\n",
    "\n",
    "        x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        x_v = self.bn3(torch.sigmoid((self.fc3(x_v))))\n",
    "        x_v = (torch.sigmoid((self.fc33(x_v))))\n",
    "\n",
    "        x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        x_w = self.bn3_w(torch.sigmoid((self.fc3_w(x_w))))\n",
    "        x_w = (torch.sigmoid((self.fc33_w(x_w))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.sigmoid(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "\n",
    "        variance = torch.sigmoid(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = (torch.mean(variance, 2))+1e-5\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.relu(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.sum(weights, 2)+1\n",
    "\n",
    "        return weights, mean, variance\n",
    "\n",
    "class DIS_MOG_new(nn.Module):\n",
    "    def __init__(self, rand, HIDDEN, dim):\n",
    "        super(DIS_MOG_new, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc1_w = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 1\n",
    "        self.sum_dim_var = 1\n",
    "        self.sum_dim_weights = 1\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        x = self.bn2_(torch.sigmoid((self.fc2_(x))))\n",
    "\n",
    "        # x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        # x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        x_m = self.bn3_(torch.sigmoid((self.fc3_(x))))\n",
    "        x_m = (torch.sigmoid((self.fc33_(x_m))))\n",
    "\n",
    "        # x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        # x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        x_v = self.bn3(torch.sigmoid((self.fc3(x))))\n",
    "        x_v = (torch.sigmoid((self.fc33(x_v))))\n",
    "\n",
    "        # x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        # x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        x_w = self.bn3_w(torch.sigmoid((self.fc3_w(x))))\n",
    "        x_w = (torch.sigmoid((self.fc33_w(x_w))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.sigmoid(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "\n",
    "        variance = torch.sigmoid(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = (torch.mean(variance, 2))+1e-6\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.mean(weights, 2)\n",
    "\n",
    "        return weights, mean, variance\n",
    "\n",
    "def generate_fix_discrete(bs, d_class):\n",
    "    num = bs/d_class\n",
    "    return torch.cat([torch.nn.functional.one_hot(torch.arange(0, d_class))]*int(num), 0)\n",
    "\n",
    "def compute_VR(learned_weights, learned_mean, MEAN):\n",
    "\n",
    "  learned_weights = learned_weights/np.mean(learned_weights)\n",
    "  return np.mean(learned_weights*((((learned_mean.reshape(-1, 1, 2) - MEAN.reshape(1, -1, 2))**2).sum(2).min(1))<1e-4))\n",
    "\n",
    "################################################## SGM\n",
    "\n",
    "class DIS_MOG_relu(nn.Module):\n",
    "    def __init__(self, rand, HIDDEN, dim):\n",
    "        super(DIS_MOG_relu, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc1_w = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 1024\n",
    "        self.sum_dim_var = 1024\n",
    "        self.sum_dim_weights = 1024\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.bn1_(torch.relu((self.fc1_(x))))\n",
    "        x = self.bn2_(torch.relu((self.fc2_(x))))\n",
    "        #x = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "\n",
    "        # x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        # x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        #x_m = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "        x_m = (torch.relu((self.fc33_(x))))\n",
    "\n",
    "        # x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        # x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        #x_v = self.bn3(torch.relu((self.fc3(x))))\n",
    "        x_v = (torch.relu((self.fc33(x))))\n",
    "\n",
    "        # x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        # x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        #x_w = self.bn3_w(torch.relu((self.fc3_w(x))))\n",
    "        x_w = (torch.relu((self.fc33_w(x))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.sigmoid(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "\n",
    "        variance = torch.sigmoid(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = (torch.mean(variance, 2))+1e-5\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.mean(weights, 2)\n",
    "\n",
    "        return weights, mean, variance\n",
    "\n",
    "def compute_pair_wise(input, MEAN_3, VARIANCE_3, WEIGHTS_3, c_t, beta_3= 0.9):\n",
    "    MEAN_DATA = (input.view(bs, 1, 2) - MEAN_3.view(1, bs, 2)).view(-1, 2)\n",
    "    VARIANCE_DATA = (VAR_ZERO + VARIANCE_3.view(1, bs, 2)).view(-1, 2)\n",
    "    WEIGHT_DATA = (WEIGHT_ZERO + WEIGHTS_3.view(1, bs)).view(-1)\n",
    "    \n",
    "#     MEAN_DATA = input - MEAN_3\n",
    "#     VARIANCE_DATA = VARIANCE_3\n",
    "    \n",
    "    numerator = torch.mean(WEIGHT_DATA*gaussian_nd_pytorch(MEAN_DATA, VARIANCE_DATA))\n",
    "    c_t = beta_3*c_t + (1-beta_3)*numerator.detach()\n",
    "    numerator_unbiased = c_t/(1-beta_3**i)\n",
    "\n",
    "    return numerator_unbiased.item(), c_t.item()\n",
    "\n",
    "def QMI_NETWORK(c1, c2, c3, c4):\n",
    "\n",
    "  shuffle_1 = torch.randperm(MEAN_3.size()[0])\n",
    "  shuffle_2 = torch.randperm(MEAN_3.size()[0])\n",
    "\n",
    "  mean_x = MEAN_3[:,  0][shuffle_1]\n",
    "  var_x = VARIANCE_3[:,  0][shuffle_1]\n",
    "  weigh_x = WEIGHTS_3[:][shuffle_1]\n",
    "\n",
    "  mean_y = MEAN_3[:,  1][shuffle_2]\n",
    "  var_y = VARIANCE_3[:,  1][shuffle_2]\n",
    "  weigh_y = WEIGHTS_3[:][shuffle_2]\n",
    "\n",
    "  MEAN_3_SHUFFLE = torch.cat((mean_x.reshape(-1, 1), mean_y.reshape(-1, 1)), 1)\n",
    "  VAR_3_SHUFFLE = torch.cat((var_x.reshape(-1, 1), var_y.reshape(-1, 1)), 1)\n",
    "  WEIGH_3_SHUFFLE = (weigh_x.reshape(-1, 1)*weigh_y.reshape(-1, 1))\n",
    "\n",
    "  PXY_GXY, c1 = compute_pair_wise(joint_, MEAN_3, VARIANCE_3, WEIGHTS_3, c1)\n",
    "  PXY_GXGY, c2 = compute_pair_wise(joint_, MEAN_3_SHUFFLE, VAR_3_SHUFFLE, WEIGH_3_SHUFFLE, c2)\n",
    "  PXPY_GXY, c3 = compute_pair_wise(disjoint_, MEAN_3, VARIANCE_3, WEIGHTS_3, c3)\n",
    "  PXPY_GXGY, c4 = compute_pair_wise(disjoint_, MEAN_3_SHUFFLE, VAR_3_SHUFFLE, WEIGH_3_SHUFFLE, c4)\n",
    "\n",
    "  return -np.log2(np.sqrt((PXPY_GXY*PXY_GXGY)/(PXPY_GXGY*PXY_GXY))), c1, c2, c3, c4\n",
    "\n",
    "for samplesss in [10, 50, 500, 1000, 3000, 5000, 10000, 30000, 50000, 100000]:\n",
    "  print('Samples Size:', samplesss)\n",
    "  x = samples_list[-2]\n",
    "  np.random.seed(4)\n",
    "  np.random.shuffle(x)\n",
    "  x = x[0:samplesss]\n",
    "\n",
    "  QMI_LIST = []\n",
    "\n",
    "  SEED = 4\n",
    "\n",
    "  torch.manual_seed(SEED)\n",
    "  np.random.seed(SEED)\n",
    "\n",
    "  rand = 1\n",
    "  HIDDEN = 256\n",
    "  dim = 1\n",
    "\n",
    "  d_class = 300\n",
    "  num = 3\n",
    "  bs = d_class*num\n",
    "\n",
    "  d_howmany = 1\n",
    "\n",
    "  MOG_NET = DIS_MOG_relu(rand+d_class*d_howmany, HIDDEN, dim*2).cuda()\n",
    "\n",
    "  optimizer = optim.Adam([\n",
    "              {'params': MOG_NET.parameters(), 'lr': 0.0001, 'betas': (0.9, 0.999)},\n",
    "          ])\n",
    "\n",
    "  entropy_list = []\n",
    "  discrete_prob = torch.ones((d_class,)).float().cuda()\n",
    "\n",
    "  beta = 0.999\n",
    "  v_t = 0.\n",
    "\n",
    "  beta_2 = 0.999\n",
    "  c_t = 0.\n",
    "\n",
    "  VAR_ZERO = torch.zeros((bs, 1, 2)).cuda()\n",
    "  WEIGHT_ZERO = torch.zeros((bs, 1)).cuda()\n",
    "\n",
    "  c1 = 0\n",
    "  c2 = 0\n",
    "  c3 = 0\n",
    "  c4 = 0\n",
    "\n",
    "  QMI_LIST = []\n",
    "\n",
    "  discrete_vec = generate_fix_discrete(bs, d_class).float().cuda()\n",
    "\n",
    "  for i in range(1, 20000):\n",
    "      uniform_vector_3 = torch.cat((torch.rand(bs, rand).cuda(), discrete_vec), 1)\n",
    "      WEIGHTS_3, MEAN_3, VARIANCE_3 = MOG_NET(uniform_vector_3)\n",
    "      \n",
    "      b1 = np.random.choice(x.shape[0], bs)\n",
    "      b2 = np.random.choice(x.shape[0], bs)\n",
    "      b3 = np.random.choice(x.shape[0], bs)\n",
    "      \n",
    "      joint_ = torch.from_numpy(x[b1, :]).float().cuda()\n",
    "      disjoint_ = torch.from_numpy(np.concatenate((x[b2, :dim], x[b3, dim:]), 1)).float().cuda()\n",
    "      \n",
    "      input = joint_\n",
    "      \n",
    "      MEAN_DIFF = (MEAN_3.view(bs, 1, 2) - MEAN_3.view(1, bs, 2)).view(-1, 2)\n",
    "      VARIANCE_SUM = (VARIANCE_3.view(bs, 1, 2) + VARIANCE_3.view(1, bs, 2)).view(-1, 2)\n",
    "      WEIGHT_DIFF = (WEIGHTS_3.view(bs, 1)*WEIGHTS_3.view(1, bs)).view(-1)\n",
    "\n",
    "      square_term = torch.mean(WEIGHT_DIFF*gaussian_nd_pytorch(MEAN_DIFF, VARIANCE_SUM))\n",
    "      v_t = beta*v_t + (1-beta)*square_term.detach()\n",
    "      square_term_unbiased = torch.sqrt(v_t/(1-beta**i))\n",
    "      \n",
    "      MEAN_DATA = (input.view(bs, 1, 2) - MEAN_3.view(1, bs, 2)).view(-1, 2)\n",
    "      VARIANCE_DATA = (VAR_ZERO + VARIANCE_3.view(1, bs, 2)).view(-1, 2)\n",
    "      WEIGHT_DATA = (WEIGHT_ZERO + WEIGHTS_3.view(1, bs)).view(-1)\n",
    "            \n",
    "      numerator = torch.mean(WEIGHT_DATA*gaussian_nd_pytorch(MEAN_DATA, VARIANCE_DATA))\n",
    "      c_t = beta_2*c_t + (1-beta_2)*numerator.detach()\n",
    "      numerator_unbiased = c_t/(1-beta_2**i)\n",
    "      \n",
    "      corr_ = (numerator/square_term_unbiased) - 0.5*numerator_unbiased*square_term/(square_term_unbiased)**3\n",
    "\n",
    "      (-corr_).backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      entropy_list.append((numerator_unbiased/square_term_unbiased).item())\n",
    "\n",
    "      with torch.no_grad():\n",
    "        QMI, c1, c2, c3, c4 = QMI_NETWORK(c1, c2, c3, c4)\n",
    "        QMI_LIST.append(QMI)\n",
    "\n",
    "      if i%100 == 0:\n",
    "\n",
    "          print('Iteration:{0}, Learned QMI:{1}, Ground Truth QMI:{2}, Cross-Entropy:{3}'.format(i, QMI_LIST[-1], 0.5357792715466267, entropy_list[-1]))\n",
    "          \n",
    "          plt.rcParams[\"figure.figsize\"] = [4,4]\n",
    "\n",
    "          learned_mean = MEAN_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "          learned_variance = VARIANCE_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "          learned_weights = WEIGHTS_3.detach().cpu().numpy()\n",
    "\n",
    "  learned_mean = MEAN_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "  learned_variance = VARIANCE_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "  learned_weights = WEIGHTS_3.detach().cpu().numpy()\n",
    "\n",
    "  print('Sample Size:{0}, Cross-Entropy:{1}, QMI-MEAN:{2}, QMI-STD:{3}'.format(samplesss, entropy_list[-1], np.mean(QMI_LIST[-3000:]), np.std(QMI_LIST[-3000:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) MINE - SHANNON'S MI \n",
    "(warning: it can diverge, so early stopping at 50000 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ MINE\n",
    "\n",
    "# QMI_LIST VARIABLE IS FOR CONVINIENCY. MINE is estimating Shannon's MI\n",
    "\n",
    "class NET(nn.Module):\n",
    "    def __init__(self, N, HIDDEN, M):\n",
    "        super(NET, self).__init__()\n",
    "        self.fc1 = nn.Linear(N, HIDDEN, bias=True)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc4 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc3 = nn.Linear(HIDDEN, M, bias=True)\n",
    "        #self.fc3 = nn.Linear(N, M, bias=True)\n",
    "        #self.fc4 = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (torch.relu((self.fc1(x))))\n",
    "        x = (torch.relu((self.fc2(x))))\n",
    "        x = (torch.relu((self.fc4(x))))\n",
    "        x = torch.sum(torch.relu(((self.fc3(x)))), 1)\n",
    "        #x = self.fc4(torch.mean(torch.sigmoid(((self.fc3(x)))), 1).view(-1, 1))\n",
    "        return x\n",
    "\n",
    "class BWM(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super(BWM, self).__init__()\n",
    "        self.fc3 = nn.Linear(N, M, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(torch.sigmoid(((self.fc3(x)))), 1)\n",
    "        return x\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "  return np.random.uniform(min, max, batch)\n",
    "\n",
    "def run_MINE(x):\n",
    "\n",
    "  QMI_LIST = []\n",
    "\n",
    "  total_dim = 100\n",
    "  K = 20 # how many Gaussian\n",
    "\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  #max = np.max(x)\n",
    "  #min = np.min(x)\n",
    "\n",
    "  # max = 1\n",
    "  # min = 0\n",
    "  #iter = 2000\n",
    "\n",
    "  net = NET(x.shape[1]*2, 256, 256).cuda()\n",
    "  #net = BWM(x.shape[1]*2, 500).cuda()\n",
    "\n",
    "  optimizer = optim.Adam([\n",
    "                {'params': net.parameters(), 'lr': 0.0001},\n",
    "            ])\n",
    "\n",
    "  bs =2000\n",
    "\n",
    "  beta_1 = 0.999\n",
    "  beta_2 = 0.999\n",
    "\n",
    "  m_t = 0\n",
    "  v_t = 0\n",
    "\n",
    "  for i in range(1, 50000):\n",
    "\n",
    "    b1 = np.random.choice(x.shape[0], bs)\n",
    "    b2 = np.random.choice(x.shape[0], bs)\n",
    "    b3 = np.random.choice(x.shape[0], bs)\n",
    "\n",
    "    joint_ = torch.from_numpy(x[b1, :]).float().cuda()\n",
    "    disjoint_ = torch.from_numpy(np.concatenate((x[b2, 0:dim], x[b3, dim:]), 1)).float().cuda()\n",
    "\n",
    "    input_1 = torch.cat((joint_, disjoint_), 1)\n",
    "    input_2 = torch.cat((disjoint_, joint_), 1)\n",
    "\n",
    "    # input = torch.cat((joint_, disjoint_), 1)\n",
    "\n",
    "    # uniform_samples = sample_uniform_data(min, max, bs*dim*4).reshape(-1, dim*4)\n",
    "    # uniform_samples = torch.from_numpy(uniform_samples).float().cuda()\n",
    "\n",
    "    b4 = np.random.choice(x.shape[0], bs)\n",
    "    b5 = np.random.choice(x.shape[0], bs)\n",
    "    b6 = np.random.choice(x.shape[0], bs)\n",
    "    b7 = np.random.choice(x.shape[0], bs)\n",
    "\n",
    "    uniform_samples = torch.from_numpy(np.concatenate((x[b4, 0:dim], x[b5, dim:], x[b6, 0:dim], x[b7, dim:]), 1)).float().cuda()\n",
    "\n",
    "    # get normal output\n",
    "    output_joint = net(input_1)\n",
    "    output_uniform = net(uniform_samples)\n",
    "\n",
    "    m_t = beta_1*m_t + (1-beta_1)*torch.mean(output_joint.detach())\n",
    "    joint_m = m_t/(1-beta_1**i)\n",
    "\n",
    "    v_t = beta_2*v_t + (1-beta_2)*torch.mean(torch.exp(output_uniform)).detach()\n",
    "    variance = v_t/(1-beta_2**i)\n",
    "    \n",
    "    #corr_ = (torch.mean(output_joint)/(torch.mean(output_uniform**2)**(1/2)))\n",
    "\n",
    "    #### RENYI DIVERGENCE\n",
    "    #corr_ = torch.mean(output_joint)/std - joint_m*torch.mean(output_uniform**2)/(2*std**3)\n",
    "    #(-corr_).backward()\n",
    "\n",
    "    ### KL DIVERGENCE\n",
    "    # corr_ = torch.mean(output_joint) - (torch.mean(torch.exp(output_uniform)))/variance.item()\n",
    "    # (-corr_).backward()\n",
    "\n",
    "    corr_ = torch.mean(output_joint) - torch.log(torch.mean(torch.exp(output_uniform)))\n",
    "    (-corr_).backward()\n",
    "\n",
    "    #GD(net, 1)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    #compute = torch.mean(output_joint)/variance.item()\n",
    "    QMI_LIST.append(corr_.item())\n",
    "\n",
    "    if i%1000 == 0:\n",
    "      print('Sample Size:{0}, Iteration:{1}, MI:{2}'.format(samplesss, i, QMI_LIST[-1]))\n",
    "\n",
    "  return QMI_LIST\n",
    "\n",
    "for samplesss in [10, 50, 500, 1000, 3000, 5000, 10000, 30000, 50000, 100000]:\n",
    "\n",
    "    x = samples_list[-2]\n",
    "    np.random.seed(4)\n",
    "    np.random.shuffle(x)\n",
    "    x = x[0:samplesss]\n",
    "\n",
    "    QMI_LIST = run_MINE(x)\n",
    "    print('Sample Size:{0}, QMI-MEAN:{2}, QMI-STD:{3}'.format(samplesss, np.mean(QMI_LIST[-10000:]), np.std(QMI_LIST[-10000:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) MINE - RENYI'S MI (ORDER 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MINE JPQ\n",
    "\n",
    "class NET(nn.Module):\n",
    "    def __init__(self, N, HIDDEN, M):\n",
    "        super(NET, self).__init__()\n",
    "        self.fc1 = nn.Linear(N, HIDDEN, bias=True)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc4 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc3 = nn.Linear(HIDDEN, M, bias=True)\n",
    "        #self.fc3 = nn.Linear(N, M, bias=True)\n",
    "        #self.fc4 = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (torch.relu((self.fc1(x))))\n",
    "        x = (torch.relu((self.fc2(x))))\n",
    "        x = (torch.relu((self.fc4(x))))\n",
    "        x = torch.sum(torch.relu(((self.fc3(x)))), 1)\n",
    "        #x = self.fc4(torch.mean(torch.sigmoid(((self.fc3(x)))), 1).view(-1, 1))\n",
    "        return x\n",
    "\n",
    "class BWM(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super(BWM, self).__init__()\n",
    "        self.fc3 = nn.Linear(N, M, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(torch.sigmoid(((self.fc3(x)))), 1)\n",
    "        return x\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "  return np.random.uniform(min, max, batch)\n",
    "\n",
    "\n",
    "def run_JPQ(x):\n",
    "\n",
    "  QMI_LIST = []\n",
    "\n",
    "  total_dim = 100\n",
    "  K = 20 # how many Gaussian\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  #max = np.max(x)\n",
    "  #min = np.min(x)\n",
    "\n",
    "  # max = 1\n",
    "  # min = 0\n",
    "  iter = 5000\n",
    "\n",
    "  net = NET(x.shape[1], 256, 256).cuda()\n",
    "  #net = BWM(x.shape[1]*2, 500).cuda()\n",
    "\n",
    "  optimizer = optim.Adam([\n",
    "                {'params': net.parameters(), 'lr': 0.0001},\n",
    "            ])\n",
    "\n",
    "  bs = 2000\n",
    "\n",
    "  beta_1 = 0.999\n",
    "  beta_2 = 0.999\n",
    "\n",
    "  m_t = 0\n",
    "  v_t = 0\n",
    "\n",
    "  QMI_LIST = []\n",
    "\n",
    "  for i in range(1, 50000):\n",
    "\n",
    "    b1 = np.random.choice(x.shape[0], bs)\n",
    "    b2 = np.random.choice(x.shape[0], bs)\n",
    "    b3 = np.random.choice(x.shape[0], bs)\n",
    "\n",
    "    joint_ = torch.from_numpy(x[b1, :]).float().cuda()\n",
    "    disjoint_ = torch.from_numpy(np.concatenate((x[b2, 0:dim], x[b3, dim:]), 1)).float().cuda()\n",
    "\n",
    "    input_1 = joint_\n",
    "    uniform_samples = disjoint_\n",
    "\n",
    "    # get normal output\n",
    "    output_joint = net(input_1)\n",
    "    output_uniform = net(uniform_samples)\n",
    "\n",
    "    m_t = beta_1*m_t + (1-beta_1)*torch.mean(output_joint.detach())\n",
    "    joint_m = m_t/(1-beta_1**i)\n",
    "\n",
    "    v_t = beta_2*v_t + (1-beta_2)*torch.mean(output_uniform.detach()**2)\n",
    "    variance = v_t/(1-beta_2**i)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    #corr_ = (torch.mean(output_joint)/(torch.mean(output_uniform**2)**(1/2)))\n",
    "\n",
    "    #### RENYI DIVERGENCE\n",
    "    #corr_ = torch.mean(output_joint)/std - joint_m*torch.mean(output_uniform**2)/(2*std**3)\n",
    "    #(-corr_).backward()\n",
    "\n",
    "    #### RENYI DIVERGENCE\n",
    "    corr_ = torch.mean(output_joint)/std - joint_m*torch.mean(output_uniform**2)/(2*std**3)\n",
    "\n",
    "    ### KL DIVERGENCE\n",
    "    #corr_ = torch.mean(output_joint) - torch.log(torch.mean(torch.exp(output_uniform)))\n",
    "    (-corr_).backward()\n",
    "\n",
    "    #GD(net, 1)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    QMI_LIST.append(-2*np.log2((joint_m/std).item()))\n",
    "\n",
    "    if i%100 == 0:\n",
    "      print('Sample Size:{0}, Iteration:{1}, Renyi MI:{2}'.format(samplesss, i, -QMI_LIST[-1]/2))\n",
    "\n",
    "  return QMI_LIST\n",
    "\n",
    "for samplesss in [10, 50, 500, 1000, 3000, 5000, 10000, 30000, 50000, 100000]:\n",
    "  x = samples_list[-2]\n",
    "  np.random.seed(4)\n",
    "  np.random.shuffle(x)\n",
    "  x = x[0:samplesss]\n",
    "\n",
    "  QMI_LIST = run_JPQ(x)\n",
    "  print('Sample Size:{0}, QMI-MEAN:{2}, QMI-STD:{3}'.format(samplesss, np.mean(QMI_LIST[-10000:]), np.std(QMI_LIST[-10000:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) High Dimensional Comparison (MINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MINE\n",
    "\n",
    "class NET(nn.Module):\n",
    "    def __init__(self, N, HIDDEN, M):\n",
    "        super(NET, self).__init__()\n",
    "        self.fc1 = nn.Linear(N, HIDDEN, bias=True)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc4 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc3 = nn.Linear(HIDDEN, M, bias=True)\n",
    "        #self.fc3 = nn.Linear(N, M, bias=True)\n",
    "        #self.fc4 = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (torch.relu((self.fc1(x))))\n",
    "        x = (torch.relu((self.fc2(x))))\n",
    "        x = (torch.relu((self.fc4(x))))\n",
    "        x = torch.sum(torch.relu(((self.fc3(x)))), 1)\n",
    "        #x = self.fc4(torch.mean(torch.sigmoid(((self.fc3(x)))), 1).view(-1, 1))\n",
    "        return x\n",
    "\n",
    "class BWM(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super(BWM, self).__init__()\n",
    "        self.fc3 = nn.Linear(N, M, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(torch.sigmoid(((self.fc3(x)))), 1)\n",
    "        return x\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "  return np.random.uniform(min, max, batch)\n",
    "\n",
    "#### SAMPLE EFFICIENCY\n",
    "\n",
    "total_dim = 10000\n",
    "K = 20 # how many Gaussian\n",
    "\n",
    "np.random.seed(4)\n",
    "center_x_total = np.array([np.random.uniform(0, 1, K)]*total_dim).T\n",
    "center_y_total = np.array([np.random.uniform(0, 1, K)]*total_dim).T\n",
    "\n",
    "investigate_SAMPLE_EFFICIENCY = []\n",
    "list_TRUE_VALUE = []\n",
    "\n",
    "list = []\n",
    "for dim in [1, 5, 10, 20, 50, 100, 200, 500]:\n",
    "\n",
    "  # CENTER OF X AND Y\n",
    "  center_x = center_x_total[:, :dim]\n",
    "  center_y = center_y_total[:, :dim]\n",
    "\n",
    "  seed = 4\n",
    "  bs = 1000\n",
    "\n",
    "  var = 0.001\n",
    "\n",
    "  # JOINT DISTRIBUTION\n",
    "  COV = np.eye(dim*2)*var\n",
    "\n",
    "#   TRUE_VALUE = compute_TRUE_CSQMI_nd(center_x, center_y, COV[:dim, :dim], COV[dim:, dim:])\n",
    "#   print('TRUE VALUE:', TRUE_VALUE)\n",
    "#   list_TRUE_VALUE.append(TRUE_VALUE)\n",
    "\n",
    "#for how_many_samples in [1, 10, 100, 1000, 10000, 100000]:\n",
    "\n",
    "  how_many_samples = 1000\n",
    "\n",
    "  QMI_LIST = []\n",
    "  np.random.seed(4)\n",
    "  samples = generate_gauss_samples_nd(center_x, center_y, COV, samples_per_class=how_many_samples)\n",
    "  x_1 = samples[:, :dim]\n",
    "  x_3 = samples[:, dim:]\n",
    "\n",
    "  x = np.concatenate((x_1, x_3), 1)\n",
    "\n",
    "  ### NORMALIZATION\n",
    "  #x = x-x.min(0)\n",
    "  #x = x/x.max(0)\n",
    "\n",
    "  #plt.scatter(x_1, x_3)\n",
    "  #plt.show()\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  #max = np.max(x)\n",
    "  #min = np.min(x)\n",
    "\n",
    "  # max = 1\n",
    "  # min = 0\n",
    "  iter = 10000\n",
    "\n",
    "  net = NET(x.shape[1], 256, 256).cuda()\n",
    "  #net = BWM(x.shape[1]*2, 500).cuda()\n",
    "\n",
    "  optimizer = optim.Adam([\n",
    "                {'params': net.parameters(), 'lr': 0.0001},\n",
    "            ])\n",
    "  \n",
    "  beta_1 = 0.999\n",
    "  beta_2 = 0.999\n",
    "\n",
    "  m_t = 0\n",
    "  v_t = 0\n",
    "\n",
    "  for i in range(1, iter):\n",
    "\n",
    "    b1 = np.random.choice(x.shape[0], bs)\n",
    "    b2 = np.random.choice(x.shape[0], bs)\n",
    "    b3 = np.random.choice(x.shape[0], bs)\n",
    "\n",
    "    joint_ = torch.from_numpy(x[b1, :]).float().cuda()\n",
    "    disjoint_ = torch.from_numpy(np.concatenate((x[b2, 0:dim], x[b3, dim:]), 1)).float().cuda()\n",
    "\n",
    "    # input_1 = torch.cat((joint_, disjoint_), 1)\n",
    "    # input_2 = torch.cat((disjoint_, joint_), 1)\n",
    "\n",
    "    # # input = torch.cat((joint_, disjoint_), 1)\n",
    "\n",
    "    # # uniform_samples = sample_uniform_data(min, max, bs*dim*4).reshape(-1, dim*4)\n",
    "    # # uniform_samples = torch.from_numpy(uniform_samples).float().cuda()\n",
    "\n",
    "    # b4 = np.random.choice(x.shape[0], bs)\n",
    "    # b5 = np.random.choice(x.shape[0], bs)\n",
    "    # b6 = np.random.choice(x.shape[0], bs)\n",
    "    # b7 = np.random.choice(x.shape[0], bs)\n",
    "\n",
    "    # uniform_samples = torch.from_numpy(np.concatenate((x[b4, 0:dim], x[b5, dim:], x[b6, 0:dim], x[b7, dim:]), 1)).float().cuda()\n",
    "\n",
    "    # get normal output\n",
    "    output_joint = net(joint_)\n",
    "    output_uniform = net(disjoint_)\n",
    "\n",
    "    m_t = beta_1*m_t + (1-beta_1)*torch.mean(output_joint.detach())\n",
    "    joint_m = m_t/(1-beta_1**i)\n",
    "\n",
    "    v_t = beta_2*v_t + (1-beta_2)*torch.mean(output_uniform.detach()**2)\n",
    "    variance = v_t/(1-beta_2**i)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    #corr_ = (torch.mean(output_joint)/(torch.mean(output_uniform**2)**(1/2)))\n",
    "    corr_ = torch.mean(output_joint) - torch.log(torch.mean(torch.exp(output_uniform)))\n",
    "    (-corr_).backward()\n",
    "    QMI_LIST.append(corr_.item())\n",
    "\n",
    "    #GD(net, 1)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  QMI_LIST.append(corr_.item())\n",
    "  investigate_SAMPLE_EFFICIENCY.append(QMI_LIST)\n",
    "\n",
    "  print('Dimension:{0}, MI-MEAN:{1}, MI-STD:{2}'.format(dim, np.mean(QMI_LIST[-3000:]), np.std(QMI_LIST[-3000:])))\n",
    "  list.append((np.mean(QMI_LIST[-3000:]), np.std(QMI_LIST[-3000:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) High Dimensional Comparison (Renyi's MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self, N, HIDDEN, M):\n",
    "        super(NET, self).__init__()\n",
    "        self.fc1 = nn.Linear(N, HIDDEN, bias=True)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc4 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc3 = nn.Linear(HIDDEN, M, bias=True)\n",
    "        #self.fc3 = nn.Linear(N, M, bias=True)\n",
    "        #self.fc4 = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (torch.relu((self.fc1(x))))\n",
    "        x = (torch.relu((self.fc2(x))))\n",
    "        x = (torch.relu((self.fc4(x))))\n",
    "        x = torch.sum(torch.relu(((self.fc3(x)))), 1)\n",
    "        #x = self.fc4(torch.mean(torch.sigmoid(((self.fc3(x)))), 1).view(-1, 1))\n",
    "        return x\n",
    "\n",
    "class BWM(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super(BWM, self).__init__()\n",
    "        self.fc3 = nn.Linear(N, M, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(torch.sigmoid(((self.fc3(x)))), 1)\n",
    "        return x\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "  return np.random.uniform(min, max, batch)\n",
    "\n",
    "#### SAMPLE EFFICIENCY\n",
    "\n",
    "total_dim = 10000\n",
    "K = 20 # how many Gaussian\n",
    "\n",
    "np.random.seed(4)\n",
    "center_x_total = np.array([np.random.uniform(0, 1, K)]*total_dim).T\n",
    "center_y_total = np.array([np.random.uniform(0, 1, K)]*total_dim).T\n",
    "\n",
    "investigate_SAMPLE_EFFICIENCY = []\n",
    "list_TRUE_VALUE = []\n",
    "\n",
    "for dim in [5, 20, 50, 200, 500]:\n",
    "\n",
    "  # CENTER OF X AND Y\n",
    "  center_x = center_x_total[:, :dim]\n",
    "  center_y = center_y_total[:, :dim]\n",
    "\n",
    "  seed = 4\n",
    "  bs = 1000\n",
    "\n",
    "  var = 0.001\n",
    "\n",
    "  # JOINT DISTRIBUTION\n",
    "  COV = np.eye(dim*2)*var\n",
    "\n",
    "#   TRUE_VALUE = compute_TRUE_CSQMI_nd(center_x, center_y, COV[:dim, :dim], COV[dim:, dim:])\n",
    "#   print('TRUE VALUE:', TRUE_VALUE)\n",
    "#   list_TRUE_VALUE.append(TRUE_VALUE)\n",
    "\n",
    "# for how_many_samples in [1, 10, 100, 1000, 10000, 100000]:\n",
    "\n",
    "  how_many_samples = 1000\n",
    "\n",
    "  QMI_LIST = []\n",
    "  np.random.seed(4)\n",
    "  samples = generate_gauss_samples_nd(center_x, center_y, COV, samples_per_class=how_many_samples)\n",
    "  x_1 = samples[:, :dim]\n",
    "  x_3 = samples[:, dim:]\n",
    "\n",
    "  x = np.concatenate((x_1, x_3), 1)\n",
    "\n",
    "  ### NORMALIZATION\n",
    "  #x = x-x.min(0)\n",
    "  #x = x/x.max(0)\n",
    "\n",
    "  #plt.scatter(x_1, x_3)\n",
    "  #plt.show()\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  #max = np.max(x)\n",
    "  #min = np.min(x)\n",
    "\n",
    "  # max = 1\n",
    "  # min = 0\n",
    "  iter = 10000\n",
    "\n",
    "  net = NET(x.shape[1], 256, 256).cuda()\n",
    "  #net = BWM(x.shape[1]*2, 500).cuda()\n",
    "\n",
    "  optimizer = optim.Adam([\n",
    "                {'params': net.parameters(), 'lr': 0.0001},\n",
    "            ])\n",
    "  \n",
    "  beta_1 = 0.999\n",
    "  beta_2 = 0.999\n",
    "\n",
    "  m_t = 0\n",
    "  v_t = 0\n",
    "\n",
    "  for i in range(1, iter):\n",
    "\n",
    "    b1 = np.random.choice(x.shape[0], bs)\n",
    "    b2 = np.random.choice(x.shape[0], bs)\n",
    "    b3 = np.random.choice(x.shape[0], bs)\n",
    "\n",
    "    joint_ = torch.from_numpy(x[b1, :]).float().cuda()\n",
    "    disjoint_ = torch.from_numpy(np.concatenate((x[b2, 0:dim], x[b3, dim:]), 1)).float().cuda()\n",
    "\n",
    "    # get normal output\n",
    "    output_joint = net(joint_)\n",
    "    output_uniform = net(disjoint_)\n",
    "\n",
    "    m_t = beta_1*m_t + (1-beta_1)*torch.mean(output_joint.detach())\n",
    "    joint_m = m_t/(1-beta_1**i)\n",
    "\n",
    "    v_t = beta_2*v_t + (1-beta_2)*torch.mean(output_uniform.detach()**2)\n",
    "    variance = v_t/(1-beta_2**i)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    #corr_ = (torch.mean(output_joint)/(torch.mean(output_uniform**2)**(1/2)))\n",
    "    corr_ = torch.mean(output_joint)/std - joint_m*torch.mean(output_uniform**2)/(2*std**3)\n",
    "    (-corr_).backward()\n",
    "\n",
    "    #GD(net, 1)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    QMI_LIST.append(np.log2((joint_m/std).item()))\n",
    "\n",
    "  print('Dimension:{0}, MI-MEAN:{1}, MI-STD:{2}'.format(dim, np.mean(QMI_LIST[-3000:]), np.std(QMI_LIST[-3000:])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
