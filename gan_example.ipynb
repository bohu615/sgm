{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "    return np.random.uniform(min, max, batch)\n",
    "\n",
    "############ WITH DISCRETE\n",
    "##### NEW CENTERS\n",
    "# def gaussian_nd_pytorch(MEAN, VARIANCE):\n",
    "#     bs = VARIANCE.shape[0]\n",
    "#     dim = VARIANCE.shape[1]\n",
    "\n",
    "#     det = (VARIANCE[:, 0]*VARIANCE[:, 1])\n",
    "    \n",
    "#     product = torch.sum(((MEAN.reshape(-1, dim)*(1/VARIANCE).view(-1, dim))*MEAN.reshape(-1, dim)), 1)\n",
    "    \n",
    "#     return ((2*np.pi)**(-dim/2))*det**(-1/2)*torch.exp(-(1/2)*product)\n",
    "\n",
    "def gaussian_nd_pytorch(MEAN, VARIANCE):\n",
    "    bs = VARIANCE.shape[0]\n",
    "    dim = VARIANCE.shape[1]\n",
    "\n",
    "    det = VARIANCE[:, 0]\n",
    "    for i in range(1, dim):\n",
    "        det = det*VARIANCE[:, i]    \n",
    "        \n",
    "    product = torch.sum(((MEAN.reshape(-1, dim)*(1/VARIANCE).view(-1, dim))*MEAN.reshape(-1, dim)), 1)\n",
    "    \n",
    "    return ((2*np.pi)**(-dim/2))*det**(-1/2)*torch.exp(-(1/2)*product)\n",
    "\n",
    "def sample_c(batchsize=32, dis_category=5):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def generate_laplacian(center_x, center_y,  scale=0.01, samples_per_class=3000):\n",
    "    \n",
    "    component_ = []\n",
    "    num_class = MEAN.shape[0]\n",
    "\n",
    "    for i in range(0, num_class):\n",
    "        dim_1 = np.random.laplace(MEAN[i, 0], scale=scale, size=samples_per_class)\n",
    "        dim_2 = np.random.laplace(MEAN[i, 1], scale=scale, size=samples_per_class)\n",
    "        samples = np.array((dim_1, dim_2)).T\n",
    "        component_.append(samples)\n",
    "\n",
    "    component_ = np.array(component_).reshape(-1, MEAN.shape[1])    \n",
    "    \n",
    "    return component_\n",
    "\n",
    "def generate_uniform(center_x, center_y,  length=0.5, samples_per_class=3000):\n",
    "    component_ = []\n",
    "    \n",
    "    num_class = MEAN.shape[0]\n",
    "    for i in range(0, num_class):\n",
    "        x_1 = np.random.uniform(center_x[i], center_x[i]+length, samples_per_class)\n",
    "        x_3 = np.random.uniform(center_y[i], center_y[i]+length, samples_per_class)\n",
    "        x = np.array((x_1, x_3)).T\n",
    "        component_.append(x)\n",
    "            \n",
    "    component_ = np.concatenate(component_, 0)\n",
    "    return component_\n",
    "\n",
    "class DIS_MOG_relu(nn.Module):\n",
    "    def __init__(self, rand, HIDDEN, dim):\n",
    "        super(DIS_MOG_relu, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc1_w = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 128\n",
    "        self.sum_dim_var = 128\n",
    "        self.sum_dim_weights = 128\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.bn1_(torch.relu((self.fc1_(x))))\n",
    "        x = self.bn2_(torch.relu((self.fc2_(x))))\n",
    "        #x = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "\n",
    "        # x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        # x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        #x_m = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "        x_m = (torch.relu((self.fc33_(x))))\n",
    "\n",
    "        # x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        # x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        #x_v = self.bn3(torch.relu((self.fc3(x))))\n",
    "        x_v = (torch.relu((self.fc33(x))))\n",
    "\n",
    "        # x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        # x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        #x_w = self.bn3_w(torch.relu((self.fc3_w(x))))\n",
    "        x_w = (torch.relu((self.fc33_w(x))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.sigmoid(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "\n",
    "        variance = torch.sigmoid(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = (torch.mean(variance, 2))+1e-6\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.mean(weights, 2)\n",
    "\n",
    "        return weights, mean, variance\n",
    "\n",
    "def generate_fix_discrete(bs, d_class):\n",
    "    num = bs/d_class\n",
    "    return torch.cat([torch.nn.functional.one_hot(torch.arange(0, d_class))]*int(num), 0).float()\n",
    "\n",
    "def compute_VR(learned_weights, learned_mean, MEAN):\n",
    "    learned_weights = learned_weights/np.mean(learned_weights)\n",
    "    return np.mean(learned_weights*((((learned_mean.reshape(-1, 1, 2) - MEAN.reshape(1, -1, 2))**2).sum(2).min(1))<1e-4))\n",
    "\n",
    "def sample_discrete_class(bs=3000, how_many=5, d_class=5):\n",
    "  return torch.nn.functional.one_hot(torch.randint(d_class, (bs*how_many,)), num_classes=d_class).view(bs, -1).float()\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def compute_square_term(W_G, M_G, V_G):\n",
    "\n",
    "  bs = M_G.shape[0]\n",
    "  dim = M_G.shape[1]\n",
    "\n",
    "  MEAN_DIFF = (M_G.view(bs, 1, dim) - M_G.view(1, bs, dim)).view(-1, dim)\n",
    "  VARIANCE_DIFF = (V_G.view(bs, 1, dim) + V_G.view(1, bs, dim)).view(-1, dim)\n",
    "  WEIGHT_DIFF = (W_G.view(bs, 1)*W_G.view(1, bs)).view(-1)\n",
    "\n",
    "  return torch.mean(WEIGHT_DIFF*gaussian_nd_pytorch(MEAN_DIFF, VARIANCE_DIFF))\n",
    "\n",
    "def compute_cross_term(W_G, M_G, V_G, W_D, M_D, V_D):\n",
    "  bs = M_G.shape[0]\n",
    "  dim = M_G.shape[1]\n",
    "\n",
    "  MEAN_CROSS = (M_G.view(bs, 1, dim) - M_D.view(1, bs, dim)).view(-1, dim)\n",
    "  VARIANCE_CROSS =  (V_G.view(bs, 1, dim) + V_D.view(1, bs, dim)).view(-1, dim)\n",
    "  WEIGHT_CROSS = (W_G.view(bs, 1)*W_D.view(1, bs)).view(-1)\n",
    "\n",
    "  return torch.mean(WEIGHT_CROSS*gaussian_nd_pytorch(MEAN_CROSS, VARIANCE_CROSS))\n",
    "\n",
    "def compute_cross_per_sample(W_G, M_G, V_G, W_D, M_D, V_D):\n",
    "  bs = M_G.shape[0]\n",
    "  dim = M_G.shape[1]\n",
    "\n",
    "  MEAN_CROSS = M_G - M_D\n",
    "  VARIANCE_CROSS =  V_G + V_D\n",
    "  WEIGHT_CROSS = (W_G*W_D).view(-1)\n",
    "\n",
    "  return torch.mean(WEIGHT_CROSS*gaussian_nd_pytorch(MEAN_CROSS, VARIANCE_CROSS))\n",
    "\n",
    "def adaptive_estimation(v_t, beta, square_term, i):\n",
    "    v_t = beta*v_t + (1-beta)*square_term.detach()\n",
    "    return v_t, (v_t/(1-beta**i))\n",
    "\n",
    "# def adaptive_estimation(v_t, beta, square_term, i):\n",
    "#     if i==1:\n",
    "#         return square_term.detach(), square_term.detach()\n",
    "#     new = beta*v_t + (1-beta)*square_term.detach()\n",
    "#     return new, new\n",
    "\n",
    "### NORMALIZE INPUT\n",
    "def normalize(M_G_1, V_G_1):\n",
    "    mean_g = torch.mean(M_G_1)\n",
    "    M_G_1 = M_G_1 - mean_g\n",
    "    M_G_1 = M_G_1/torch.std(M_G_1)\n",
    "    #std_g = torch.mean(V_G_1+torch.mean(M_G_1**2))\n",
    "    #V_G_1 = V_G_1/std_g\n",
    "    #M_G_1 = M_G_1/std_g\n",
    "    return M_G_1, V_G_1\n",
    "\n",
    "def normalize_nogradient(M_G_1, V_G_1):\n",
    "    M_G_1 = M_G_1 - mean_g.detach()\n",
    "    M_G_1 = M_G_1/torch.std(M_G_1).detach()\n",
    "    return M_G_1, V_G_1\n",
    "\n",
    "def gaussian_1d(input, m, sigma):\n",
    "    det = sigma\n",
    "    inv = 1/sigma\n",
    "        \n",
    "    input = input.reshape(-1, 1)\n",
    "    m = m.reshape(1, -1)\n",
    "\n",
    "    return ((2*np.pi)**(-1/2))*det**(-1/2)*np.exp(-(1/2)*((input-m)**2*inv))\n",
    "\n",
    "def construct_contour1d(centers, weights, learned_variance, interp=100):\n",
    "    QMI_TRUE_LIST = []\n",
    "    min = 0\n",
    "    max = 1\n",
    "    delta = (max-min)/interp\n",
    "\n",
    "    x_axis = np.linspace(min, max, interp)\n",
    "\n",
    "    gaussian_plot_joint_ = []\n",
    "    gaussian_plot_split_x_ = []\n",
    "    gaussian_plot_split_y_ = []\n",
    "\n",
    "    gaussian_plot_joint_ = (weights*gaussian_1d(x_axis, centers, learned_variance))/np.sum(weights)\n",
    "    gaussian_plot_joint = np.mean(np.array(gaussian_plot_joint_), 1)/delta\n",
    "    \n",
    "    return gaussian_plot_joint\n",
    "\n",
    "def visualize():\n",
    "    learned_mean = M_G_1.detach().cpu().numpy().reshape(-1)\n",
    "    learned_variance = V_G_1.detach().cpu().numpy().reshape(-1)\n",
    "    learned_weights = W_G_1.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "    gaussian_plot_joint = construct_contour1d(learned_mean, learned_weights, learned_variance, interp=100)\n",
    "    plt.plot(gaussian_plot_joint, label='G')\n",
    "\n",
    "    learned_mean = M_D_1.detach().cpu().numpy().reshape(-1)\n",
    "    learned_variance = V_D_1.detach().cpu().numpy().reshape(-1)\n",
    "    learned_weights = W_D_1.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "    gaussian_plot_joint = construct_contour1d(learned_mean, learned_weights, learned_variance, interp=100)\n",
    "    plt.plot(gaussian_plot_joint, label='D')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "class avgpool(nn.Module):\n",
    "    def __init__(self, up_size=0):\n",
    "        super(avgpool, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "        return out_man\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if resample == 'up' or resample == 'up_nbn':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.upsample = torch.nn.Upsample(scale_factor=2)\n",
    "            self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "        elif resample == 'down':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            #self.pool = avgpool()\n",
    "            self.pool = torch.nn.AvgPool2d(2, 2)\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "        elif resample == None:\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            \n",
    "        self.resample = resample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.resample == None:\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up_nbn':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            #output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            #output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "                        \n",
    "        elif self.resample == 'down':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "            shortcut = self.pool(shortcut)\n",
    "\n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)    #convmeanpool\n",
    "            output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock_thefirstone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        #self.pool = avgpool()\n",
    "        self.pool = torch.nn.AvgPool2d(2, 2)\n",
    "        self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x\n",
    "        output = x\n",
    "        \n",
    "        shortcut = self.pool(shortcut) #meanpoolconv\n",
    "        shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        #output = self.bn1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.conv2(output) #convmeanpool\n",
    "        output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "#create_gan_architecture\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand, 2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        #x = x.reshape(x.shape[0], 30, 3, x.shape[2], x.shape[3])\n",
    "        #x = nn.functional.sigmoid(x)\n",
    "        #x = torch.tanh(x)\n",
    "\n",
    "        x = torch.relu(x)\n",
    "        x = torch.clamp(x, 0, 1)\n",
    "\n",
    "#         x = torch.relu(x)\n",
    "#         x = torch.clamp(x, 0, 1)\n",
    "        #x = x.mean(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim_input=3*32*32, dim=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dim = dim\n",
    "        HIDDEN = 2048\n",
    "    \n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(dim_input, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN, affine=False)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN, affine=False)\n",
    "        \n",
    "        self.fc01_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc02_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc03_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.bn4_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.bn5_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 1024\n",
    "        self.sum_dim_var = 1024\n",
    "        self.sum_dim_weights = 1024\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, 3*32*32)\n",
    "\n",
    "        x = self.bn1_(torch.relu((self.fc1_(x))))\n",
    "        x = self.bn2_(torch.relu((self.fc2_(x))))\n",
    "        #x = (torch.sigmoid((self.fc01_(x))))\n",
    "\n",
    "        # x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        # x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        #x_m = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "        x_m = (torch.relu((self.fc33_(x))))\n",
    "\n",
    "        # x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        # x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        #x_v = self.bn3(torch.relu((self.fc3(x))))\n",
    "        x_v = (torch.relu((self.fc33(x))))\n",
    "\n",
    "        # x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        # x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        #x_w = self.bn3_w(torch.relu((self.fc3_w(x))))\n",
    "        x_w = (torch.relu((self.fc33_w(x))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.relu(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "        mean = torch.clamp(mean, 1e-5, 1)\n",
    "\n",
    "        variance = torch.relu(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = torch.mean(variance, 2)\n",
    "        variance = torch.clamp(variance, 1e-3, 1)*0+1e-2\n",
    "        #variance = torch.clamp(variance, 1e-6, 1e-1)\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.relu(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.mean(weights, 2)\n",
    "        #weights = torch.clamp(weights, 1e-5)\n",
    "\n",
    "        return mean\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand, 2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, nc, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        #x = x.reshape(x.shape[0], 30, 3, x.shape[2], x.shape[3])\n",
    "        #x = nn.functional.sigmoid(x)\n",
    "        #x = torch.tanh(x)\n",
    "\n",
    "        x = torch.relu(x)\n",
    "        x = torch.clamp(x, 0, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim_input=1*32*32, dim=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dim = dim\n",
    "        HIDDEN = 2048\n",
    "    \n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(dim_input, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN, affine=False)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN, affine=False)\n",
    "        \n",
    "        self.fc01_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc02_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.fc03_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.bn4_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.bn5_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 1024\n",
    "        self.sum_dim_var = 1024\n",
    "        self.sum_dim_weights = 1024\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, 1*32*32)\n",
    "\n",
    "        x = self.bn1_(torch.relu((self.fc1_(x))))\n",
    "        x = self.bn2_(torch.relu((self.fc2_(x))))\n",
    "        #x = (torch.sigmoid((self.fc01_(x))))\n",
    "\n",
    "        # x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        # x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        #x_m = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "        x_m = (torch.relu((self.fc33_(x))))\n",
    "\n",
    "        # x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        # x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        #x_v = self.bn3(torch.relu((self.fc3(x))))\n",
    "        x_v = (torch.relu((self.fc33(x))))\n",
    "\n",
    "        # x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        # x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        #x_w = self.bn3_w(torch.relu((self.fc3_w(x))))\n",
    "        x_w = (torch.relu((self.fc33_w(x))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.relu(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "        mean = torch.clamp(mean, 0, 1)\n",
    "\n",
    "        variance = torch.relu(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = torch.mean(variance, 2)\n",
    "        variance = torch.clamp(variance, 1e-3, 1)*0+1e-2\n",
    "        #variance = torch.clamp(variance, 1e-6, 1e-1)\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.relu(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.mean(weights, 2)\n",
    "        #weights = torch.clamp(weights, 1e-5)\n",
    "\n",
    "        return mean\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((32,32)), transforms.ToTensor()])\n",
    "\n",
    "bs = 100\n",
    "\n",
    "# trainset = torchvision.datasets.MNIST(root='./MNIST/', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.MNIST(root='./MNIST/', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./fashion-mnist/', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./fashion-mnist/', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "nc = 1\n",
    "ndf = 32\n",
    "ngf = 64\n",
    "\n",
    "beta = 0.9\n",
    "v_t = 0.\n",
    "\n",
    "beta_2 = 0.999\n",
    "v_t_D = 0.\n",
    "\n",
    "beta_3 = 0.9\n",
    "v_t_g = 0.\n",
    "\n",
    "beta_4 = 0.999\n",
    "v_t_D_g = 0.\n",
    "\n",
    "entropy_list = []\n",
    "\n",
    "i = 1\n",
    "\n",
    "SEED = 4\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "rand = 100\n",
    "output_dim = 32*32*3\n",
    "\n",
    "dim = 1\n",
    "\n",
    "d_class = 10\n",
    "how_many = 10\n",
    "\n",
    "def uniform(stdev, size):\n",
    "    return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "def initialize_conv(m,he_init=True):\n",
    "    fan_in = m.in_channels * m.kernel_size[0]**2\n",
    "    fan_out = m.out_channels * m.kernel_size[0]**2 / (m.stride[0]**2)\n",
    "\n",
    "    if m.kernel_size[0]==3:\n",
    "        filters_stdev = np.sqrt(4./(fan_in+fan_out))\n",
    "    # Normalized init (Glorot & Bengio)\n",
    "    else: \n",
    "        filters_stdev = np.sqrt(2./(fan_in+fan_out))\n",
    "        \n",
    "    filter_values = uniform(\n",
    "                    filters_stdev,\n",
    "                    (m.kernel_size[0], m.kernel_size[0], m.in_channels, m.out_channels)\n",
    "                )\n",
    "    \n",
    "    return filter_values\n",
    "\n",
    "def initialize_linear(m):\n",
    "    weight_values = uniform(\n",
    "                np.sqrt(2./(m.in_features+m.out_features)),\n",
    "                (m.in_features, m.out_features)\n",
    "            )\n",
    "    return weight_values\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight = torch.from_numpy(initialize_conv(m))\n",
    "        #m.weight.data.copy_(weight.permute(3,2,1,0))\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_values = torch.from_numpy(initialize_linear(m))\n",
    "        m.weight.data.copy_(weight_values.transpose(0,1))\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "gen_net = Generator(d_class*how_many+rand).cuda()\n",
    "dis_net = Discriminator().cuda()\n",
    "\n",
    "# gen_net.apply(weights_init)\n",
    "# dis_net.apply(weights_init)\n",
    "\n",
    "optimizer_g = optim.Adam([\n",
    "            {'params': gen_net.parameters(), 'lr': 1e-5, 'betas': (0.5, 0.999)},\n",
    "        ])\n",
    "\n",
    "optimizer_d = optim.Adam([\n",
    "    \n",
    "            {'params': dis_net.parameters(), 'lr': 1e-5, 'betas': (0.5, 0.999)},\n",
    "        ])\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "# trainloader = torch.utils.data.DataLoader(new_dataset, batch_size=10000,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "dataiter = iter(trainloader)\n",
    "#####\n",
    "fixed_input, _ = dataiter.next()\n",
    "x = fixed_input.numpy()[:2]\n",
    "\n",
    "iters = 40000\n",
    "iter_d = 1\n",
    "iter_g = 1\n",
    "\n",
    "fixed_noise_ = torch.cat((torch.rand(bs, rand).cuda(), sample_discrete_class(bs, how_many, d_class).cuda()), 1) \n",
    "vtrack = []\n",
    "vdtrack = []\n",
    "\n",
    "for j in range(1, 5555555):\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    while 1:\n",
    "        for param in gen_net.parameters():\n",
    "          param.requires_grad = True\n",
    "        for param in dis_net.parameters():\n",
    "          param.requires_grad = True\n",
    "        \n",
    "        #gen_net.eval()\n",
    "\n",
    "        # sample inputs\n",
    "        uniform_vector_3 = torch.cat((torch.rand(bs, rand).cuda(), sample_discrete_class(bs, how_many, d_class).cuda()), 1) \n",
    "        \n",
    "        try:\n",
    "            input, labels = dataiter.next()\n",
    "            input = input.float().cuda()\n",
    "        except:\n",
    "            dataiter = iter(trainloader)\n",
    "            input, labels = dataiter.next()\n",
    "            input = input.float().cuda()\n",
    "        \n",
    "        x_generated = gen_net(uniform_vector_3)\n",
    "        \n",
    "        M_G_1 = dis_net(x_generated)\n",
    "        M_D_1 = dis_net(input)\n",
    "\n",
    "        top = torch.mean(M_G_1)\n",
    "        down = torch.mean(M_D_1**2)\n",
    "        \n",
    "        v_t, mean_cons = adaptive_estimation(v_t, 0.9, top, i)\n",
    "        v_t_D, quadratic_cons = adaptive_estimation(v_t_D, 0.999, down, i)\n",
    "        \n",
    "        vtrack.append(v_t.item())\n",
    "        vdtrack.append(v_t_D.item())\n",
    "        corr_ = top/torch.sqrt(quadratic_cons) - 0.5*mean_cons*down/torch.sqrt(quadratic_cons**3)\n",
    "        \n",
    "        ((-corr_)).backward()\n",
    "                        \n",
    "        for p in gen_net.parameters():\n",
    "            p.grad = -p.grad  # or whatever other operation\n",
    "\n",
    "        optimizer_g.step()\n",
    "        optimizer_d.step()\n",
    "        optimizer_g.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        i+=1\n",
    "        iter_d+=1\n",
    "        k+=1\n",
    "        \n",
    "        if k>1:\n",
    "            break\n",
    "\n",
    "    entropy_list.append((mean_cons/torch.sqrt(quadratic_cons)).item())\n",
    "    \n",
    "    if j%10 == 0:\n",
    "        print(j, entropy_list[-1])\n",
    "        \n",
    "        plt.rcParams[\"figure.figsize\"] = [4,4]\n",
    "\n",
    "        x_generated = gen_net(fixed_noise_)\n",
    "        vutils.save_image(x_generated[0:100, :3, :, :].data, './fake_fmnist.png', nrow=10, normalize=True)\n",
    "        img = mpimg.imread('fake_fmnist.png')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        np.save('./model/model_gan_SGM', gen_net)\n",
    "        np.save('./model/model_gan_d_SGM', dis_net)\n",
    "        \n",
    "        plt.rcParams[\"figure.figsize\"] = [4,4]\n",
    "        \n",
    "        plt.title('Density Ratio')\n",
    "        plt.hist(M_G_1.detach().cpu().view(-1), bins=100, label='Generated data')\n",
    "        plt.hist(M_D_1.detach().cpu().view(-1), bins=100, label='Real data')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
