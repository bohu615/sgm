{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def construct_contour_gauss(centers, weights, learned_variance, interp=100):\n",
    "    QMI_TRUE_LIST = []\n",
    "    interp = 100\n",
    "    delta = (max-min)/interp\n",
    "\n",
    "    x_axis = np.linspace(min, max, interp)\n",
    "    y_axis = np.linspace(min, max, interp)\n",
    "    xv, yv = np.meshgrid(x_axis,y_axis)\n",
    "\n",
    "    input = np.array((xv, yv)).reshape(2, -1).T\n",
    "\n",
    "    gaussian_plot_joint_ = []\n",
    "    gaussian_plot_split_x_ = []\n",
    "    gaussian_plot_split_y_ = []\n",
    "\n",
    "    #centers = np.concatenate((center_x, center_y), 1)\n",
    "    #difference = input.reshape(-1, 2) - centers.reshape(-1, 1, 2)\n",
    "\n",
    "    for i in range(0, centers.shape[0]):\n",
    "        gaussian_plot_joint_.append(weights[i]*gaussian_nd(input - centers[i], 0, learned_variance[i]))\n",
    "    gaussian_plot_joint = np.mean(np.array(gaussian_plot_joint_), 0)*delta*delta\n",
    "    \n",
    "    return gaussian_plot_joint.reshape(interp, interp)\n",
    "\n",
    "def gaussian_nd_numpy(MEAN, VARIANCE):\n",
    "    bs = VARIANCE.shape[0]\n",
    "    dim = VARIANCE.shape[1]\n",
    "\n",
    "    det = np.linalg.det(VARIANCE)\n",
    "    inv = np.linalg.pinv(VARIANCE)\n",
    "            \n",
    "    product = np.sum((MEAN.reshape(-1, 1, dim)@inv).reshape(-1, dim)*MEAN.reshape(-1, dim), 1)\n",
    "    \n",
    "    return ((2*np.pi)**(-dim/2))*det**(-1/2)*np.exp(-(1/2)*product)\n",
    "\n",
    "def compute_TRUE_ENTROPY(MEAN_matrix, COV_matrix, weights_matrix):\n",
    "    K = MEAN_matrix.shape[0]\n",
    "    dim = MEAN_matrix.shape[1]\n",
    "    \n",
    "    MEAN_DIFF = MEAN_matrix.reshape(K, 1, dim) - MEAN_matrix.reshape(1, K, dim)\n",
    "    COV_DIFF = COV_matrix.reshape(K, 1, dim, dim) + COV_matrix.reshape(1, K, dim, dim)\n",
    "    WEIGHT_DIFF = weights_matrix.reshape(K, 1)*weights_matrix.reshape(1, K)\n",
    "    \n",
    "    return np.sqrt(np.sum(WEIGHT_DIFF.reshape(-1)*gaussian_nd_numpy(MEAN_DIFF.reshape(-1, dim), COV_DIFF.reshape(-1, dim, dim))))\n",
    "\n",
    "def gaussian_nd(input, m, sigma):\n",
    "    k = sigma.shape[0]\n",
    "    det = np.linalg.det(sigma)\n",
    "    inv = np.linalg.pinv(sigma)\n",
    "    \n",
    "    return ((2*np.pi)**(-k/2))*det**(-1/2)*np.exp(-(1/2)*np.sum((input-m)@inv*(input-m), 1))\n",
    "\n",
    "#### CONSTRUCT A MORE RESONABLE MKM\n",
    "\n",
    "min = 0\n",
    "max = 1\n",
    "\n",
    "def makediag3d(a):\n",
    "    a = np.asarray(a)\n",
    "    depth, size = a.shape\n",
    "    x = np.zeros((depth,size,size))\n",
    "    for i in range(depth):\n",
    "        x[i].flat[slice(0,None,1+size)] = a[i]\n",
    "    return x\n",
    "\n",
    "def create_Gaussian_mixture_MC():\n",
    "    np.random.seed(4)\n",
    "    \n",
    "    num = 10\n",
    "    center_x = np.linspace(0.2, 0.8, num)\n",
    "    center_y = np.linspace(0.2, 0.8, num)\n",
    "    \n",
    "    np.random.shuffle(center_x)\n",
    "    np.random.shuffle(center_y)\n",
    "\n",
    "    xv, yv = np.meshgrid(center_x, center_y)\n",
    "\n",
    "    MEAN_MATRIX = np.array((xv, yv)).reshape(2, -1).T\n",
    "    COV = makediag3d(np.random.uniform(0.0005, 0.002, MEAN_MATRIX.shape[0]*2).reshape(num*num, 2))        \n",
    "    weights = np.ones((num, num))*(0.3/(num-1))+np.eye(num)*(0.7-0.3/(num-1))\n",
    "    \n",
    "    return MEAN_MATRIX, COV, weights, center_x, center_y\n",
    "\n",
    "MEAN_matrix, COV_matrix, weights_matrix, center_x, center_y = create_Gaussian_mixture_MC()\n",
    "COV_matrix = COV_matrix*3\n",
    "weights_matrix = weights_matrix.reshape(-1)\n",
    "\n",
    "pdf = construct_contour_gauss(MEAN_matrix, weights_matrix.reshape(-1), COV_matrix)\n",
    "normalize = pdf/np.sum(pdf,1).reshape(-1, 1)\n",
    "\n",
    "print('Generating pdf for EXP #1...')\n",
    "\n",
    "plt.imshow(pdf, origin='lower', extent=[min, max, min, max])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(normalize, origin='lower', extent=[min, max, min, max])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_1d(input, m, sigma):\n",
    "    det = sigma\n",
    "    inv = 1/sigma\n",
    "        \n",
    "    input = input.reshape(-1, 1)\n",
    "    m = m.reshape(1, -1)\n",
    "\n",
    "    return ((2*np.pi)**(-1/2))*det**(-1/2)*np.exp(-(1/2)*((input-m)**2*inv))\n",
    "\n",
    "def generate_gauss_samples_various(MEAN, COV_matrix, samples_per_class=3000000):\n",
    "    \n",
    "    num_class = MEAN.shape[0]\n",
    "    component_ = []\n",
    "    for i in range(0, num_class):\n",
    "        COV = COV_matrix[i]\n",
    "        samples = np.random.normal(MEAN[i], np.sqrt(COV), int(samples_per_class))\n",
    "        component_.append(samples)\n",
    "    component_ = np.array(component_)\n",
    "    \n",
    "    return component_\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "inter = 100\n",
    "\n",
    "x_0 = np.linspace(0.1, 0.9, inter)\n",
    "#x_0 = np.array([0.5])\n",
    "next_samples = np.copy(x_0)\n",
    "density = []\n",
    "stored_samples = generate_gauss_samples_various(MEAN_matrix[:, 0], COV_matrix[:, 0, 0])\n",
    "\n",
    "iter = 1000\n",
    "\n",
    "current_ = np.zeros((iter, inter))\n",
    "next_ = np.zeros((iter, inter))\n",
    "num_samples = np.zeros((weights_matrix.shape[0]), dtype=int)\n",
    "\n",
    "print('Start generating samples...')\n",
    "\n",
    "for i in range(0, iter):\n",
    "    current_[i] = np.copy(next_samples)\n",
    "    weights_x0 = weights_matrix*gaussian_1d(next_samples, MEAN_matrix[:, 1], (COV_matrix[:, 1, 1]))\n",
    "    weights_x0 = weights_x0/np.sum(weights_x0, 1).reshape(-1, 1)\n",
    "\n",
    "    next_chosen = np.array([np.random.choice(weights_.shape[0], 1, p=weights_)[0] for weights_ in weights_x0])\n",
    "    next_samples = []\n",
    "    for j in next_chosen:\n",
    "        next_samples.append(stored_samples[j, num_samples[j]])\n",
    "        num_samples[j]+=1\n",
    "    next_samples = np.array(next_samples)\n",
    "    density.append(np.histogram(next_samples, bins=100)[0])\n",
    "    \n",
    "    next_[i] = np.copy(next_samples)\n",
    "    \n",
    "print('Done')\n",
    "joint_samples = np.array((current_, next_)).reshape(2, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.reload_library()\n",
    "\n",
    "# plt.style.use('science')\n",
    "# plt.style.use(['science','no-latex', 'high-vis', 'notebook'])\n",
    "# plt.rcParams[\"figure.figsize\"] = [6,4]\n",
    "\n",
    "# WHETHER TO COMMENT\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "#torch.cuda.set_device(1)\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "    return np.random.uniform(min, max, batch)\n",
    "\n",
    "def gaussian_nd_pytorch(MEAN, VARIANCE):\n",
    "    bs = VARIANCE.shape[0]\n",
    "    dim = VARIANCE.shape[1]\n",
    "\n",
    "    det = VARIANCE[:, 0]\n",
    "    for i in range(1, dim):\n",
    "        det = det*VARIANCE[:, i]    \n",
    "        \n",
    "    product = torch.sum(((MEAN.reshape(-1, dim)*(1/VARIANCE).view(-1, dim))*MEAN.reshape(-1, dim)), 1)\n",
    "    \n",
    "    return ((2*np.pi)**(-dim/2))*det**(-1/2)*torch.exp(-(1/2)*product)\n",
    "\n",
    "def sample_c(batchsize=32, dis_category=5):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def generate_laplacian(center_x, center_y,  scale=0.01, samples_per_class=3000):\n",
    "    \n",
    "    component_ = []\n",
    "    num_class = MEAN.shape[0]\n",
    "\n",
    "    for i in range(0, num_class):\n",
    "        dim_1 = np.random.laplace(MEAN[i, 0], scale=scale, size=samples_per_class)\n",
    "        dim_2 = np.random.laplace(MEAN[i, 1], scale=scale, size=samples_per_class)\n",
    "        samples = np.array((dim_1, dim_2)).T\n",
    "        component_.append(samples)\n",
    "\n",
    "    component_ = np.array(component_).reshape(-1, MEAN.shape[1])    \n",
    "    \n",
    "    return component_\n",
    "\n",
    "def generate_uniform(center_x, center_y,  length=0.5, samples_per_class=3000):\n",
    "    component_ = []\n",
    "    \n",
    "    num_class = MEAN.shape[0]\n",
    "    for i in range(0, num_class):\n",
    "        x_1 = np.random.uniform(center_x[i], center_x[i]+length, samples_per_class)\n",
    "        x_3 = np.random.uniform(center_y[i], center_y[i]+length, samples_per_class)\n",
    "        x = np.array((x_1, x_3)).T\n",
    "        component_.append(x)\n",
    "            \n",
    "    component_ = np.concatenate(component_, 0)\n",
    "    return component_\n",
    "\n",
    "class DIS_MOG_relu(nn.Module):\n",
    "    def __init__(self, rand, HIDDEN, dim):\n",
    "        super(DIS_MOG_relu, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc1_w = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 128\n",
    "        self.sum_dim_var = 128\n",
    "        self.sum_dim_weights = 128\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.bn1_(torch.relu((self.fc1_(x))))\n",
    "        x = self.bn2_(torch.relu((self.fc2_(x))))\n",
    "        #x = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "\n",
    "        # x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        # x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        #x_m = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "        x_m = (torch.relu((self.fc33_(x))))\n",
    "\n",
    "        # x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        # x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        #x_v = self.bn3(torch.relu((self.fc3(x))))\n",
    "        x_v = (torch.relu((self.fc33(x))))\n",
    "\n",
    "        # x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        # x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        #x_w = self.bn3_w(torch.relu((self.fc3_w(x))))\n",
    "        x_w = (torch.relu((self.fc33_w(x))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.sigmoid(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "\n",
    "        variance = torch.sigmoid(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = (torch.mean(variance, 2))*0.1+1e-6\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.mean(weights, 2)\n",
    "\n",
    "        return weights, mean, variance\n",
    "\n",
    "def gaussian_nd(input, m, sigma):\n",
    "    k = sigma.shape[0]\n",
    "    det = np.linalg.det(sigma)\n",
    "    inv = np.linalg.pinv(sigma)\n",
    "    \n",
    "    return ((2*np.pi)**(-k/2))*det**(-1/2)*np.exp(-(1/2)*np.sum((input-m)@inv*(input-m), 1))\n",
    "\n",
    "def construct_contour1d(centers, weights, learned_variance, interp=100):\n",
    "    QMI_TRUE_LIST = []\n",
    "    min = 0\n",
    "    max = 1\n",
    "    delta = (max-min)/interp\n",
    "\n",
    "    x_axis = np.linspace(min, max, interp)\n",
    "\n",
    "    gaussian_plot_joint_ = []\n",
    "    gaussian_plot_split_x_ = []\n",
    "    gaussian_plot_split_y_ = []\n",
    "\n",
    "    gaussian_plot_joint_ = (weights*gaussian_1d(x_axis, centers, learned_variance))/np.sum(weights)\n",
    "    gaussian_plot_joint = np.mean(np.array(gaussian_plot_joint_), 1)/delta\n",
    "    \n",
    "    return gaussian_plot_joint\n",
    "\n",
    "def visualization_():\n",
    "    \n",
    "    MOG_NET.eval()\n",
    "    d_class = 300\n",
    "    num = 1\n",
    "    bs = d_class*num\n",
    "    discrete_vec = generate_fix_discrete(bs, d_class).float().cuda()\n",
    "\n",
    "    uniform_vector_3 = torch.cat((torch.rand(bs, rand).cuda(), discrete_vec), 1)\n",
    "    scanning_input = torch.from_numpy(np.linspace(0, 1, 100)).float().cuda()\n",
    "\n",
    "    interp = scanning_input.reshape(-1, 1).shape[0]\n",
    "    uniform_vector_3_ = uniform_vector_3.reshape(bs, 1, -1).repeat(1, interp, 1)\n",
    "    scanning_input_ = scanning_input.reshape(1, -1, 1).repeat(bs, 1, 1)\n",
    "    input = torch.cat((uniform_vector_3_, scanning_input_), 2).reshape(bs*interp, -1)\n",
    "    WEIGHTS_3, MEAN_3, VARIANCE_3 = MOG_NET(input)    \n",
    "\n",
    "    learned_mean = MEAN_3.detach().cpu().numpy().reshape(bs, interp)\n",
    "    learned_variance = VARIANCE_3.detach().cpu().numpy().reshape(bs, interp)\n",
    "    learned_weights = WEIGHTS_3.detach().cpu().numpy().reshape(bs, interp)\n",
    "\n",
    "    gaussian_plot_joint_ = []\n",
    "    for i in range(0, interp):\n",
    "        gaussian_plot_joint = construct_contour1d(learned_mean[:, i], learned_weights[:, i], learned_variance[:, i], interp=100)\n",
    "        #plt.plot(np.linspace(0, 1, 100), gaussian_plot_joint)\n",
    "        gaussian_plot_joint_.append(gaussian_plot_joint)\n",
    "\n",
    "    plt.contour(np.array(gaussian_plot_joint_), origin='lower', extent=[min, max, min, max])\n",
    "    plt.show()\n",
    "    \n",
    "    MOG_NET.train()\n",
    "    \n",
    "torch.manual_seed(6)\n",
    "np.random.seed(4)\n",
    "\n",
    "iter = 200000\n",
    "rand = 1\n",
    "HIDDEN = 256\n",
    "dim = 1\n",
    "\n",
    "d_class = 300\n",
    "num = 20\n",
    "bs = d_class*num\n",
    "\n",
    "d_howmany = 1\n",
    "\n",
    "MOG_NET = DIS_MOG_relu(rand+d_class*d_howmany+dim, HIDDEN, dim).cuda()\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "            {'params': MOG_NET.parameters(), 'lr': 0.001, 'betas': (0.9, 0.999)},\n",
    "        ])\n",
    "\n",
    "entropy_list = []\n",
    "discrete_prob = torch.ones((d_class,)).float().cuda()\n",
    "\n",
    "beta = 0.999\n",
    "v_t = 0.\n",
    "\n",
    "beta_2 = 0.999\n",
    "c_t = 0.\n",
    "\n",
    "x = current_[:].reshape(-1)\n",
    "y = next_[:].reshape(-1)\n",
    "\n",
    "VAR_ZERO = torch.zeros((bs, 1, 1)).cuda()\n",
    "WEIGHT_ZERO = torch.zeros((bs, 1)).cuda()\n",
    "\n",
    "def generate_fix_discrete(bs, d_class):\n",
    "    num = bs/d_class\n",
    "    return torch.cat([torch.nn.functional.one_hot(torch.arange(0, d_class))]*int(num), 0)\n",
    "\n",
    "discrete_vec = generate_fix_discrete(bs, d_class).float().cuda()\n",
    "\n",
    "for i in range(1, iter):\n",
    "    \n",
    "    b1 = np.random.choice(x.shape[0], bs)\n",
    "    x_i = torch.from_numpy(x[b1].reshape(-1, 1)).float().cuda()\n",
    "    y_i = torch.from_numpy(y[b1].reshape(-1, 1)).float().cuda()\n",
    "        \n",
    "    uniform_vector_3 = torch.cat((torch.rand(bs, rand).cuda(), discrete_vec), 1)\n",
    "    uniform_vector_3 = uniform_vector_3[torch.randperm(uniform_vector_3.size()[0])]\n",
    "    \n",
    "    uniform_vector_3 = torch.cat((uniform_vector_3, x_i), 1)\n",
    "\n",
    "    uniform_vector_4 = torch.cat((torch.rand(bs, rand).cuda(), discrete_vec), 1)\n",
    "    uniform_vector_4 = uniform_vector_4[torch.randperm(uniform_vector_4.size()[0])]\n",
    "\n",
    "    uniform_vector_4 = torch.cat((uniform_vector_4, x_i), 1)\n",
    "\n",
    "    WEIGHTS_3, MEAN_3, VARIANCE_3 = MOG_NET(uniform_vector_3)    \n",
    "    WEIGHTS_4, MEAN_4, VARIANCE_4 = MOG_NET(uniform_vector_4)    \n",
    "\n",
    "    MEAN_DATA = torch.cat((y_i - MEAN_3, y_i - MEAN_4))\n",
    "    VARIANCE_DATA = torch.cat((VARIANCE_3, VARIANCE_4))  \n",
    "    WEIGHT_DATA = torch.cat((WEIGHTS_3, WEIGHTS_4))  \n",
    "    \n",
    "    MEAN_DIFF = MEAN_3 - MEAN_4\n",
    "    VARIANCE_SUM = VARIANCE_3 + VARIANCE_4\n",
    "    WEIGHT_DIFF = WEIGHTS_3*WEIGHTS_4\n",
    "    \n",
    "    square_term = torch.mean(WEIGHT_DIFF*gaussian_nd_pytorch(MEAN_DIFF, VARIANCE_SUM))\n",
    "    v_t = beta*v_t + (1-beta)*square_term.detach()\n",
    "    square_term_unbiased = torch.sqrt(v_t/(1-beta**i))\n",
    "    \n",
    "    numerator = torch.mean(WEIGHT_DATA*gaussian_nd_pytorch(MEAN_DATA, VARIANCE_DATA))\n",
    "    c_t = beta_2*c_t + (1-beta_2)*numerator.detach()\n",
    "    numerator_unbiased = c_t/(1-beta_2**i)\n",
    "    \n",
    "    corr_ = (numerator/square_term_unbiased) - 0.5*numerator_unbiased*square_term/(square_term_unbiased)**3\n",
    "    (-corr_).backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    entropy_list.append((numerator_unbiased/square_term_unbiased).item())\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        \n",
    "        plt.title('Model Conditional')\n",
    "        visualization_()\n",
    "        \n",
    "        learned_mean = MEAN_3.detach().cpu().numpy().reshape(-1, 1)\n",
    "        plt.scatter(x_i.detach().cpu(), learned_mean, s=0.1, color='black')\n",
    "        \n",
    "        learned_mean = MEAN_4.detach().cpu().numpy().reshape(-1, 1)\n",
    "        plt.scatter(x_i.detach().cpu(), learned_mean, s=0.1, color='black', label='Model Centers')\n",
    "\n",
    "        plt.plot(center_y, center_x, 'ro',color='red', label='True Main Components')\n",
    "        # plt.contour(normalize, origin='lower', extent=[min, max, min, max])\n",
    "\n",
    "        for j in range(0, center_y.shape[0]):\n",
    "            plt.text(center_y[j]+0.01, center_x[j]+0.01, str(np.where(np.argsort(center_y)==j)[0][0]+1), color='red', fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print('Iteration:', i, 'Cross-Entropy:', entropy_list[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
