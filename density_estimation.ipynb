{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATE A LOT DATASET\n",
    "min = 0\n",
    "max = 1\n",
    "\n",
    "!mkdir -p model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def laplacian_pdf(input, mean, scale):\n",
    "     return (1/(2*scale))*np.exp(-np.abs(input-mean)/scale)\n",
    "\n",
    "def gaussian_nd(input, m, sigma):\n",
    "    k = sigma.shape[0]\n",
    "    det = np.linalg.det(sigma)\n",
    "    inv = np.linalg.pinv(sigma)\n",
    "    \n",
    "    return ((2*np.pi)**(-k/2))*det**(-1/2)*np.exp(-(1/2)*np.sum((input-m)@inv*(input-m), 1))\n",
    "\n",
    "def construct_contour(centers, weights, learned_variance, interp=100):\n",
    "    QMI_TRUE_LIST = []\n",
    "    interp = 100\n",
    "    delta = (max-min)/interp\n",
    "\n",
    "    x_axis = np.linspace(min, max, interp)\n",
    "    y_axis = np.linspace(min, max, interp)\n",
    "    xv, yv = np.meshgrid(x_axis,y_axis)\n",
    "\n",
    "    input = np.array((xv, yv)).reshape(2, -1).T\n",
    "\n",
    "    gaussian_plot_joint_ = []\n",
    "    gaussian_plot_split_x_ = []\n",
    "    gaussian_plot_split_y_ = []\n",
    "\n",
    "    #centers = np.concatenate((center_x, center_y), 1)\n",
    "    difference = input.reshape(1, -1, 2) - centers.reshape(-1, 1, 2)\n",
    "\n",
    "    for i in range(0, centers.shape[0]):\n",
    "        gaussian_plot_joint_.append(weights[i]*gaussian_nd(difference[i], 0, np.array([[learned_variance[i,0], 0], [0, learned_variance[i,1]]])).reshape(interp, interp))\n",
    "\n",
    "    gaussian_plot_joint = np.mean(np.array(gaussian_plot_joint_), 0)*delta*delta\n",
    "    \n",
    "    return gaussian_plot_joint\n",
    "\n",
    "def construct_pdf_mix_mixture(center_x, center_y, COV, samples_per_class=3000):\n",
    "    \n",
    "    MEAN = np.concatenate([center_x, center_y], 1)   \n",
    "    COV_ = []\n",
    "    weights_ = []\n",
    "    component_ = []\n",
    "    num_class = MEAN.shape[0]\n",
    "    \n",
    "    interp = 1000\n",
    "    max = 1\n",
    "    min = 0\n",
    "    delta = (max-min)/interp\n",
    "\n",
    "    x_axis = np.linspace(min, max, interp)\n",
    "    y_axis = np.linspace(min, max, interp)\n",
    "    xv, yv = np.meshgrid(x_axis,y_axis)\n",
    "\n",
    "    input = np.array((xv, yv)).reshape(2, -1).T\n",
    "    pdf_map = np.zeros((interp, interp))\n",
    "    \n",
    "    for i in range(0, num_class):\n",
    "        COV[0, 0] = np.random.uniform(0.0001, 0.002, 1)[0]\n",
    "        COV[1, 1] = np.random.uniform(0.0001, 0.002, 1)[0]\n",
    "        weights = np.random.uniform(0, 1, 1)[0]+0.5\n",
    "        \n",
    "        rv = np.random.choice(3, 1)[0]\n",
    "        \n",
    "        if rv==0:\n",
    "            samples = np.random.multivariate_normal(MEAN[i], COV, int(weights*samples_per_class))\n",
    "            pdf_map = pdf_map + (weights*gaussian_nd(input - MEAN[i], 0, COV)).reshape(interp, interp)\n",
    "            \n",
    "        if rv==1:\n",
    "            dim_1 = np.random.uniform(MEAN[i, 0]-COV[0, 0]*10, MEAN[i, 0]+COV[0, 0]*10, int(weights*samples_per_class))\n",
    "            dim_2 = np.random.uniform(MEAN[i, 1]-COV[1, 1]*10, MEAN[i, 1]+COV[1, 1]*10, int(weights*samples_per_class))\n",
    "            samples = np.array((dim_1, dim_2)).T\n",
    "            pdf = (input[:, 0]>(MEAN[i, 0]-COV[0, 0]*10))*(input[:, 0]<(MEAN[i, 0]+COV[0, 0]*10))\n",
    "            pdf = pdf*(input[:, 1]>(MEAN[i, 1]-COV[1, 1]*10))*(input[:, 1]<(MEAN[i, 1]+COV[1, 1]*10))\n",
    "            pdf_map = pdf_map + (pdf/(400*COV[0, 0]*COV[1, 1])).reshape(interp, interp)\n",
    "            \n",
    "        if rv==2:\n",
    "            dim_1 = np.random.laplace(MEAN[i, 0], scale=COV[0, 0]*10, size=int(weights*samples_per_class))\n",
    "            dim_2 = np.random.laplace(MEAN[i, 1], scale=COV[1, 1]*10, size=int(weights*samples_per_class))\n",
    "            samples = np.array((dim_1, dim_2)).T\n",
    "            pdf_dim_1 = laplacian_pdf(input[:, 0], MEAN[i, 0], COV[0, 0]*10)\n",
    "            pdf_dim_2 = laplacian_pdf(input[:, 1], MEAN[i, 1], COV[1, 1]*10)\n",
    "            \n",
    "            pdf_map = pdf_map + weights*(pdf_dim_1*pdf_dim_2).reshape(interp, interp)\n",
    "            \n",
    "        component_.append(samples)\n",
    "        COV_.append(np.copy(COV))\n",
    "        weights_.append(np.copy(weights))\n",
    "    component_ = np.concatenate(component_).reshape(-1, COV.shape[0])\n",
    "    \n",
    "    return component_, MEAN, np.array(COV_), np.array(weights_), pdf_map, delta\n",
    "\n",
    "K = 20\n",
    "dim = 1\n",
    "\n",
    "seed_ = [4, 5, 6, 7, 8]\n",
    "for seed in seed_:\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    center_x = np.array([np.random.uniform(0.2, 0.8, K)]*dim).T\n",
    "    center_y = np.array([np.random.uniform(0.2, 0.8, K)]*dim).T\n",
    "\n",
    "    how_many_samples = 20000\n",
    "\n",
    "    var = 0.001\n",
    "\n",
    "    dim = 1\n",
    "    # JOINT DISTRIBUTION\n",
    "    COV = np.eye(dim*2)*var\n",
    "\n",
    "    QMI_LIST = []\n",
    "    np.random.seed(seed)\n",
    "    samples, MEAN, COV_, weights_, pdf_map, delta = construct_pdf_mix_mixture(center_x, center_y, COV, samples_per_class=how_many_samples)\n",
    "    pdf_map = pdf_map/np.sum(pdf_map)\n",
    "\n",
    "    true_entropy = np.sqrt(np.sum(pdf_map*pdf_map)/(delta*delta))\n",
    "    print('EXP #{0} entropy:'.format(seed-3), true_entropy)\n",
    "    plt.imshow(np.log(pdf_map+1e-5), origin='lower', extent=[min, max, min, max])\n",
    "    plt.show()\n",
    "\n",
    "# plt.style.reload_library()\n",
    "\n",
    "# plt.style.use('science')\n",
    "# plt.style.use(['science','no-latex', 'high-vis', 'notebook'])\n",
    "# plt.rcParams[\"figure.figsize\"] = [6,4]\n",
    "\n",
    "# WHETHER TO COMMENT\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "#torch.cuda.set_device(1)\n",
    "\n",
    "def GD(net, lr):\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data - lr*param.grad\n",
    "            \n",
    "    net.zero_grad()\n",
    "    return 0\n",
    "\n",
    "def sample_uniform_data(min=-5, max=5, batch=300):\n",
    "    return np.random.uniform(min, max, batch)\n",
    "\n",
    "def gaussian_nd_pytorch(MEAN, VARIANCE):\n",
    "    bs = VARIANCE.shape[0]\n",
    "    dim = VARIANCE.shape[1]\n",
    "\n",
    "    det = VARIANCE[:, 0]\n",
    "    for i in range(1, dim):\n",
    "        det = det*VARIANCE[:, i]    \n",
    "        \n",
    "    product = torch.sum(((MEAN.reshape(-1, dim)*(1/VARIANCE).view(-1, dim))*MEAN.reshape(-1, dim)), 1)\n",
    "    \n",
    "    return ((2*np.pi)**(-dim/2))*det**(-1/2)*torch.exp(-(1/2)*product)\n",
    "\n",
    "def sample_c(batchsize=32, dis_category=5):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def generate_laplacian(center_x, center_y,  scale=0.01, samples_per_class=3000):\n",
    "    \n",
    "    component_ = []\n",
    "    num_class = MEAN.shape[0]\n",
    "\n",
    "    for i in range(0, num_class):\n",
    "        dim_1 = np.random.laplace(MEAN[i, 0], scale=scale, size=samples_per_class)\n",
    "        dim_2 = np.random.laplace(MEAN[i, 1], scale=scale, size=samples_per_class)\n",
    "        samples = np.array((dim_1, dim_2)).T\n",
    "        component_.append(samples)\n",
    "\n",
    "    component_ = np.array(component_).reshape(-1, MEAN.shape[1])    \n",
    "    \n",
    "    return component_\n",
    "\n",
    "def generate_uniform(center_x, center_y,  length=0.5, samples_per_class=3000):\n",
    "    component_ = []\n",
    "    \n",
    "    num_class = MEAN.shape[0]\n",
    "    for i in range(0, num_class):\n",
    "        x_1 = np.random.uniform(center_x[i], center_x[i]+length, samples_per_class)\n",
    "        x_3 = np.random.uniform(center_y[i], center_y[i]+length, samples_per_class)\n",
    "        x = np.array((x_1, x_3)).T\n",
    "        component_.append(x)\n",
    "            \n",
    "    component_ = np.concatenate(component_, 0)\n",
    "    return component_\n",
    "\n",
    "class DIS_MOG_relu(nn.Module):\n",
    "    def __init__(self, rand, HIDDEN, dim):\n",
    "        super(DIS_MOG_relu, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        \n",
    "        self.fc1_ = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_ = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_ = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.fc1_w = nn.Linear(rand, HIDDEN, bias=True)\n",
    "        self.bn1_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3_w = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc33_w = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "\n",
    "        self.sum_dim_mean = 128\n",
    "        self.sum_dim_var = 128\n",
    "        self.sum_dim_weights = 128\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, dim*self.sum_dim_mean, bias=True)\n",
    "        self.fc6 = nn.Linear(HIDDEN, dim*self.sum_dim_var, bias=True)\n",
    "        self.fcw = nn.Linear(HIDDEN, self.sum_dim_weights, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.bn1_(torch.relu((self.fc1_(x))))\n",
    "        x = self.bn2_(torch.relu((self.fc2_(x))))\n",
    "        #x = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "\n",
    "        # x_m = self.bn1_(torch.sigmoid((self.fc1_(x))))\n",
    "        # x_m = self.bn2_(torch.sigmoid((self.fc2_(x_m))))\n",
    "        #x_m = self.bn3_(torch.relu((self.fc3_(x))))\n",
    "        x_m = (torch.relu((self.fc33_(x))))\n",
    "\n",
    "        # x_v = self.bn1(torch.sigmoid((self.fc1(x))))\n",
    "        # x_v = self.bn2(torch.sigmoid((self.fc2(x_v))))\n",
    "        #x_v = self.bn3(torch.relu((self.fc3(x))))\n",
    "        x_v = (torch.relu((self.fc33(x))))\n",
    "\n",
    "        # x_w = self.bn1_w(torch.sigmoid((self.fc1_w(x))))\n",
    "        # x_w = self.bn2_w(torch.sigmoid((self.fc2_w(x_w))))\n",
    "        #x_w = self.bn3_w(torch.relu((self.fc3_w(x))))\n",
    "        x_w = (torch.relu((self.fc33_w(x))))\n",
    "\n",
    "        dim = self.dim \n",
    "        \n",
    "        mean = torch.sigmoid(self.fc5(x_m)).view(x.shape[0], dim, self.sum_dim_mean)\n",
    "        mean = torch.mean(mean, 2)\n",
    "\n",
    "        variance = torch.sigmoid(self.fc6(x_v)).view(x.shape[0], dim, self.sum_dim_var)\n",
    "        variance = (torch.mean(variance, 2))+1e-6\n",
    "        \n",
    "        # weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        # weights = torch.mean(weights, 2)\n",
    "\n",
    "        weights = torch.sigmoid(self.fcw(x_w)).view(x.shape[0], 1, self.sum_dim_weights)\n",
    "        weights = torch.mean(weights, 2)\n",
    "\n",
    "        return weights, mean, variance\n",
    "\n",
    "def generate_fix_discrete(bs, d_class):\n",
    "    num = bs/d_class\n",
    "    return torch.cat([torch.nn.functional.one_hot(torch.arange(0, d_class))]*int(num), 0)\n",
    "\n",
    "def compute_VR(learned_weights, learned_mean, MEAN):\n",
    "\n",
    "  learned_weights = learned_weights/np.mean(learned_weights)\n",
    "  return np.mean(learned_weights*((((learned_mean.reshape(-1, 1, 2) - MEAN.reshape(1, -1, 2))**2).sum(2).min(1))<1e-4))\n",
    "\n",
    "def run_SGM(SEED, x, MEAN, iter=60000):\n",
    "  torch.manual_seed(SEED)\n",
    "  np.random.seed(SEED)\n",
    "\n",
    "  rand = 1\n",
    "  HIDDEN = 1024\n",
    "  dim = 1\n",
    "\n",
    "  d_class = 300\n",
    "  num = 20\n",
    "  bs = d_class*num\n",
    "\n",
    "  d_howmany = 1\n",
    "\n",
    "  MOG_NET = DIS_MOG_relu(rand+d_class*d_howmany, HIDDEN, dim*2).cuda()\n",
    "\n",
    "  optimizer = optim.Adam([\n",
    "              {'params': MOG_NET.parameters(), 'lr': 0.001, 'betas': (0.9, 0.999)},\n",
    "          ])\n",
    "\n",
    "  entropy_list = []\n",
    "  discrete_prob = torch.ones((d_class,)).float().cuda()\n",
    "\n",
    "  beta = 0.999\n",
    "  v_t = 0.\n",
    "\n",
    "  beta_2 = 0.999\n",
    "  c_t = 0.\n",
    "\n",
    "  VAR_ZERO = torch.zeros((bs, 1, 2)).cuda()\n",
    "  WEIGHT_ZERO = torch.zeros((bs, 1)).cuda()\n",
    "\n",
    "  discrete_vec = generate_fix_discrete(bs, d_class).float().cuda()\n",
    "\n",
    "  for i in range(1, iter):\n",
    "  #     if i%1000 == 0 and bs<11000:\n",
    "  #         bs+=1000\n",
    "  #         VAR_ZERO = torch.zeros((bs, 1, 2)).cuda()\n",
    "  #         WEIGHT_ZERO = torch.zeros((bs, 1)).cuda()\n",
    "\n",
    "  #     uniform_vector_1 = torch.cat((torch.rand(bs, rand).cuda(), \n",
    "  #         torch.nn.functional.one_hot(torch.multinomial(discrete_prob, d_howmany*bs, replacement=True), d_class).view(bs, d_howmany*d_class).float()), 1)\n",
    "  #     WEIGHTS_1, MEAN_1, VARIANCE_1 = MOG_NET(uniform_vector_1)\n",
    "      \n",
    "  #     uniform_vector_2 = torch.cat((torch.rand(bs, rand).cuda(), \n",
    "  #         torch.nn.functional.one_hot(torch.multinomial(discrete_prob, d_howmany*bs, replacement=True), d_class).view(bs, d_howmany*d_class).float()), 1)\n",
    "  #     WEIGHTS_2, MEAN_2, VARIANCE_2 = MOG_NET(uniform_vector_2)\n",
    "      \n",
    "  #     uniform_vector_3 = torch.cat((torch.rand(bs, rand).cuda(), \n",
    "  #         torch.nn.functional.one_hot(torch.multinomial(discrete_prob, d_howmany*bs, replacement=True), d_class).view(bs, d_howmany*d_class).float()), 1)\n",
    "  #     WEIGHTS_3, MEAN_3, VARIANCE_3 = MOG_NET(uniform_vector_3)\n",
    "      #WEIGHTS_3.fill_(1)\n",
    "      \n",
    "      uniform_vector_3 = torch.cat((torch.rand(bs, rand).cuda(), discrete_vec), 1)\n",
    "      WEIGHTS_3, MEAN_3, VARIANCE_3 = MOG_NET(uniform_vector_3)\n",
    "      \n",
    "      b1 = np.random.choice(x.shape[0], bs)\n",
    "      b2 = np.random.choice(x.shape[0], bs)\n",
    "      b3 = np.random.choice(x.shape[0], bs)\n",
    "      \n",
    "      joint_ = torch.from_numpy(x[b1, :]).float().cuda()\n",
    "      disjoint_ = torch.from_numpy(np.concatenate((x[b2, :dim], x[b3, dim:]), 1)).float().cuda()\n",
    "      \n",
    "      input = joint_\n",
    "      \n",
    "      MEAN_DIFF = (MEAN_3.view(bs, 1, 2) - MEAN_3.view(1, bs, 2)).view(-1, 2)\n",
    "      VARIANCE_SUM = (VARIANCE_3.view(bs, 1, 2) + VARIANCE_3.view(1, bs, 2)).view(-1, 2)\n",
    "      WEIGHT_DIFF = (WEIGHTS_3.view(bs, 1)*WEIGHTS_3.view(1, bs)).view(-1)\n",
    "\n",
    "  #     MEAN_DIFF = MEAN_1 - MEAN_3\n",
    "  #     VARIANCE_SUM = VARIANCE_1 + VARIANCE_3\n",
    "\n",
    "      square_term = torch.mean(WEIGHT_DIFF*gaussian_nd_pytorch(MEAN_DIFF, VARIANCE_SUM))\n",
    "      v_t = beta*v_t + (1-beta)*square_term.detach()\n",
    "      square_term_unbiased = torch.sqrt(v_t/(1-beta**i))\n",
    "      \n",
    "      MEAN_DATA = (input.view(bs, 1, 2) - MEAN_3.view(1, bs, 2)).view(-1, 2)\n",
    "      VARIANCE_DATA = (VAR_ZERO + VARIANCE_3.view(1, bs, 2)).view(-1, 2)\n",
    "      WEIGHT_DATA = (WEIGHT_ZERO + WEIGHTS_3.view(1, bs)).view(-1)\n",
    "      \n",
    "  #     MEAN_DATA = input - MEAN_3\n",
    "  #     VARIANCE_DATA = VARIANCE_3\n",
    "      \n",
    "      numerator = torch.mean(WEIGHT_DATA*gaussian_nd_pytorch(MEAN_DATA, VARIANCE_DATA))\n",
    "      c_t = beta_2*c_t + (1-beta_2)*numerator.detach()\n",
    "      numerator_unbiased = c_t/(1-beta_2**i)\n",
    "      \n",
    "      corr_ = (numerator/square_term_unbiased) - 0.5*numerator_unbiased*square_term/(square_term_unbiased)**3\n",
    "      #corr_ = -square_term + 2*numerator\n",
    "\n",
    "      (-corr_).backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      entropy_list.append((numerator_unbiased/square_term_unbiased).item())\n",
    "      \n",
    "      if i%10 == 0:\n",
    "          \n",
    "          plt.rcParams[\"figure.figsize\"] = [4,4]\n",
    "\n",
    "          learned_mean = MEAN_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "          learned_variance = VARIANCE_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "          learned_weights = WEIGHTS_3.detach().cpu().numpy()\n",
    "\n",
    "  #         WEIGHTS_3 = WEIGHTS_3.detach().cpu().numpy()\n",
    "          \n",
    "          # plt.scatter(learned_mean[0:,0], learned_mean[0:,1], s=1, c=learned_weights[:,0], zorder=333)\n",
    "          # joint = joint_.detach().cpu()\n",
    "          # plt.scatter(joint[:,0], joint[:,1], s=1, alpha=0.6, color='pink')\n",
    "          # plt.show()\n",
    "\n",
    "          print('Iteration:',i, 'Cross-entropy:', entropy_list[-1])\n",
    "          np.save('./model/MOG_NET_AM_seed{0}.npy'.format(SEED), MOG_NET)\n",
    "          np.save('./model/entropy_list_AM_seed{0}.npy'.format(SEED), entropy_list)\n",
    "\n",
    "          #print(i,'2',(numerator/torch.sqrt(square_term)).item())\n",
    "\n",
    "  learned_mean = MEAN_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "  learned_variance = VARIANCE_3.detach().cpu().numpy().reshape(-1, 2)\n",
    "  learned_weights = WEIGHTS_3.detach().cpu().numpy()\n",
    "\n",
    "  np.save('./model/MOG_NET_AM_seed{0}.npy'.format(SEED), MOG_NET)\n",
    "  np.save('./model/entropy_list_AM_seed{0}.npy'.format(SEED), entropy_list)\n",
    "  rate = compute_VR(learned_weights, learned_mean, MEAN)\n",
    "\n",
    "  print('seed: {0}, entropy: {1}, validation rate:{2}'.format(SEED, entropy_list[-1], rate))\n",
    "\n",
    "seed_ = [4, 5, 6, 7, 8]\n",
    "for seed in seed_:\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    center_x = np.array([np.random.uniform(0.2, 0.8, K)]*dim).T\n",
    "    center_y = np.array([np.random.uniform(0.2, 0.8, K)]*dim).T\n",
    "\n",
    "    how_many_samples = 20000\n",
    "\n",
    "    var = 0.001\n",
    "\n",
    "    dim = 1\n",
    "    # JOINT DISTRIBUTION\n",
    "    COV = np.eye(dim*2)*var\n",
    "\n",
    "    QMI_LIST = []\n",
    "    np.random.seed(seed)\n",
    "    samples, MEAN, COV_, weights_, pdf_map, delta = construct_pdf_mix_mixture(center_x, center_y, COV, samples_per_class=how_many_samples)\n",
    "    pdf_map = pdf_map/np.sum(pdf_map)\n",
    "\n",
    "    true_entropy = np.sqrt(np.sum(pdf_map*pdf_map)/(delta*delta))\n",
    "    print('EXP #{0} entropy:'.format(seed-3), true_entropy)\n",
    "    plt.imshow(np.log(pdf_map+1e-5), origin='lower', extent=[min, max, min, max])\n",
    "    plt.show()\n",
    "\n",
    "    run_SGM(seed, samples, MEAN, iter=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
